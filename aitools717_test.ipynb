{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/calllummm/AI-Tools-in-Student-Learning/blob/main/aitools717_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbgl22hYQeLC"
      },
      "source": [
        "# Exploratory Data Analysis: AI Tools in Student Learning Codebase and Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-w2rLTYeQixc"
      },
      "source": [
        "Based on 114 responses collected so far, this is an observation of the dataset that has accumulated over three weeks. As this continues to develop, more test runs will be performed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbARIdRaQxOF"
      },
      "source": [
        "**IMPORTING DATA**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IlRX9tUEQzhU"
      },
      "outputs": [],
      "source": [
        "# libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 38
        },
        "id": "Wep1xm2WQ3jr",
        "outputId": "e8eb3520-c1cb-4876-a563-f05678b965e4"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-59136462-c735-4c1d-9167-850a8e84224b\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-59136462-c735-4c1d-9167-850a8e84224b\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# upload file\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3KvfcFbtR0TG"
      },
      "source": [
        "**OVERVIEW AND DATA CLEANING**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XukY-JQR3kd"
      },
      "source": [
        "After importing the dataset, we can see generally how data is organized within the document and what changes should be made so it can be interpreted by various python libraries and machine learning commands."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bz-voa3p7b4E"
      },
      "source": [
        "This data is taken directly from a survey created by Callum Magarian, a Data Science undergraduate at University of Rhode Island. This survey was approved by the Institution Review Board which evaluates ethical considerations going into mass conducted surveys. Students reflect on their experiences seeing Artificial Intelligence tools being integrated into student learning environments. There are a variety of close ended and open ended questions that are addressed with strong student responses to be analyzed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Uw0AIfaBSFkx"
      },
      "outputs": [],
      "source": [
        "# read through data\n",
        "df = pd.read_csv('aitools717.csv')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IAqaB58uY-R1"
      },
      "outputs": [],
      "source": [
        "# separate different column types\n",
        "open_ended_cols = [\n",
        "    'What suggestions or concerns do you have about AI tools being integrated into education?',\n",
        "    'Is there anything else you would like to share about your opinions and experiences regarding AI tools and student learning?',\n",
        "    'If you have experienced benefits or challenges, please describe them.',\n",
        "    'Have you noticed any impact on your academic integrity or ethical considerations when using AI tools? Please explain.',\n",
        "    'Briefly explain how you have discovered some of the most useful AI tools to you.'\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lzo-f74Hl-bT"
      },
      "outputs": [],
      "source": [
        "ordinal_cols = [\n",
        "    'What is your grade level?',\n",
        "    'How familiar are you Artificial Intelligence tools?',\n",
        "    'How long have you been using AI tools for?',\n",
        "    'How often do you use these AI tools? Please estimate how many times per week or month you use these tools.',\n",
        "    'To what degree do you agree with the following statement? \\nAI tools affected your ability to understand course material and learn independently.'\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wbVGXii2o83i"
      },
      "outputs": [],
      "source": [
        "nominal_multi_select_cols = [\n",
        "    'What Artificial Intelligence tools do you frequently use? (If you use any tools in mind that are not in the checklist, please include in the \"Other...\" section)',\n",
        "    'What types of academic tasks do you use AI tools for?',\n",
        "    'What college subjects do you often use AI tools for? Please list specific course names or subject areas.'\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t9dLI8SLmEuC"
      },
      "outputs": [],
      "source": [
        "nominal_cols = [\n",
        "    'What is your major?',\n",
        "    'What gender do you identify as?',\n",
        "    'What is your race / ethnic background?',\n",
        "    'Have you ever received instruction from professors or URI itself about using AI tools for your coursework? If so, please describe the advice or policies you have encountered.',\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHXqIoOLrvOI"
      },
      "source": [
        "Next, a necessary check to determine if there are missing values and assessing the best resulting course of action."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BcxoERv0rorH"
      },
      "outputs": [],
      "source": [
        "# check for missing data\n",
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pS4wM5vO1Obx"
      },
      "source": [
        "Below is a predictive method that determines with statistical certainty what missing data would be."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7HwTbgbGrNu"
      },
      "source": [
        "**IMPUTATION PROCESS**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GlGLYBAA-7vy"
      },
      "source": [
        "Below are some basic methods to fill in columns of different response types."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uj3154rc_pSg"
      },
      "outputs": [],
      "source": [
        "# handling open ended columns\n",
        "def impute_open_ended(df, open_ended_cols):\n",
        "    df[open_ended_cols] = df[open_ended_cols].fillna('')\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xel4pyC_AqeQ"
      },
      "outputs": [],
      "source": [
        "# handling ordinal columns\n",
        "def impute_ordinal(df, ordinal_cols, random_state=None):\n",
        "    if random_state is not None:\n",
        "        np.random.seed(random_state)\n",
        "    for col in ordinal_cols:\n",
        "        value_counts = df[col].value_counts(dropna=True, normalize=True)\n",
        "        missing = df[col].isnull()\n",
        "        n_missing = missing.sum()\n",
        "        if n_missing > 0 and not value_counts.empty:\n",
        "            sampled = np.random.choice(value_counts.index, size=n_missing, p=value_counts.values)\n",
        "            df.loc[missing, col] = sampled\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ykzm-7iDBPrJ"
      },
      "outputs": [],
      "source": [
        "# handling nominal multi select columns\n",
        "def impute_nominal_multi_select(df, nominal_multi_select_cols):\n",
        "    df[nominal_multi_select_cols] = df[nominal_multi_select_cols].fillna('')\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "886sEZ8xBTxB"
      },
      "outputs": [],
      "source": [
        "# handling nominal single select columns\n",
        "def impute_nominal(df, nominal_cols, random_state=None):\n",
        "    if random_state is not None:\n",
        "        np.random.seed(random_state)\n",
        "    for col in nominal_cols:\n",
        "        value_counts = df[col].value_counts(dropna=True, normalize=True)\n",
        "        missing = df[col].isnull()\n",
        "        n_missing = missing.sum()\n",
        "        if n_missing > 0 and not value_counts.empty:\n",
        "            sampled = np.random.choice(value_counts.index, size=n_missing, p=value_counts.values)\n",
        "            df.loc[missing, col] = sampled\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eH1SQ9rhFjHW"
      },
      "outputs": [],
      "source": [
        "# standardize missing values\n",
        "df.replace(['', 'N/A', 'na', 'NA'], np.nan, inplace=True)\n",
        "\n",
        "# imputation functions\n",
        "df = impute_open_ended(df, open_ended_cols)\n",
        "df = impute_ordinal(df, ordinal_cols, random_state=42)\n",
        "df = impute_nominal_multi_select(df, nominal_multi_select_cols)\n",
        "df = impute_nominal(df, nominal_cols, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w-StEEUzBq_3"
      },
      "outputs": [],
      "source": [
        "# any missing values?\n",
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_TFjC9nUHeJG"
      },
      "outputs": [],
      "source": [
        "df.isnull().sum()[df.isnull().sum() > 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JmDkHm-EHzr0"
      },
      "outputs": [],
      "source": [
        "df[df.isnull().any(axis=1)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-4gdDNQBJAjn"
      },
      "outputs": [],
      "source": [
        "for col in df.columns:\n",
        "    if df[col].isnull().sum() > 0:\n",
        "        value_counts = df[col].value_counts(dropna=True, normalize=True)\n",
        "        missing = df[col].isnull()\n",
        "        n_missing = missing.sum()\n",
        "        if n_missing > 0 and not value_counts.empty:\n",
        "            sampled = np.random.choice(value_counts.index, size=n_missing, p=value_counts.values)\n",
        "            df.loc[missing, col] = sampled"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W91XNAsQJBWm"
      },
      "outputs": [],
      "source": [
        "# any missing columns\n",
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KlRje88l_mKj"
      },
      "source": [
        "**SUMMARY STATISTICS**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DMp0zollG1s-"
      },
      "outputs": [],
      "source": [
        "# summary statistics\n",
        "df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OLiux6g0JxPS"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "ordinal_order = ['Not familiar', 'Somewhat familiar', 'Fairly familiar', 'Very familiar', 'Extremely familiar']\n",
        "df['How familiar are you Artificial Intelligence tools?'].value_counts().reindex(ordinal_order).plot(kind='bar')\n",
        "plt.title('Familiarity with AI Tools')\n",
        "plt.xlabel('Familiarity Level')\n",
        "plt.ylabel('Number of Respondents')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WqynVsQWD-j"
      },
      "source": [
        "There are clear, substantial issues with how the dataset's information is handled given the lack of responses generated in the visualizations above. Below will be an extensive section that covers cleaning methods for each column so that it can be accurately represented in the analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDEbqkQSYNvz"
      },
      "source": [
        "# Data Cleaning for All Columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGELOXCCMSDF"
      },
      "source": [
        "**CLEAN GRADE COLUMNS**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ggSnXmlfMU98"
      },
      "outputs": [],
      "source": [
        "# unique values of grade\n",
        "df['What is your grade level?'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZdhZM1k-FKp9"
      },
      "outputs": [],
      "source": [
        "# normalize values\n",
        "df['grade_clean'] = df['What is your grade level?'].astype(str).str.lower().str.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eMyVX3UQLdlq"
      },
      "outputs": [],
      "source": [
        "# function: classifies grades based on keywords\n",
        "def classify_grade(text):\n",
        "    if pd.isnull(text):\n",
        "        return None\n",
        "    text = text.strip().lower()\n",
        "    if 'freshman' in text or 'summer prerequisites' in text:\n",
        "        return 'freshman'\n",
        "    if 'sophomore' in text:\n",
        "        return 'sophomore'\n",
        "    if 'junior' in text or 'third year' in text or 'transfer' in text:\n",
        "        return 'junior'\n",
        "    if 'senior' in text or 'just graduated' in text or '4th year' in text:\n",
        "        return 'senior'\n",
        "    if any(word in text for word in ['grad', 'master', 'phd', 'dpt', 'bsme']):\n",
        "        return 'graduate'\n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WCly0sG0Lev7"
      },
      "outputs": [],
      "source": [
        "# apply function to dataframe\n",
        "df['grade_category'] = df['What is your grade level?'].astype(str).apply(classify_grade)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JeoZK2h0LpUw"
      },
      "outputs": [],
      "source": [
        "# numeric encoding\n",
        "grade_encoding = {\n",
        "    'freshman': 1,\n",
        "    'sophomore': 2,\n",
        "    'junior': 3,\n",
        "    'senior': 4,\n",
        "    'graduate': 5,\n",
        "}\n",
        "\n",
        "df['grade_encoded'] = df['grade_category'].map(grade_encoding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "APSNE2OxLdqY"
      },
      "outputs": [],
      "source": [
        "print(df[['What is your grade level?', 'grade_category', 'grade_encoded']])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ams3xbpAA71z"
      },
      "outputs": [],
      "source": [
        "# unique values of grade\n",
        "df['grade_category'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MkjUhb7dOV8K"
      },
      "outputs": [],
      "source": [
        "# how many nan values\n",
        "df['What is your grade level?'].isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xIQa2Mq8TJAs"
      },
      "outputs": [],
      "source": [
        "# ordinal grade distribution\n",
        "class_order= [\n",
        "    'freshman', 'sophomore', 'junior', 'senior', 'graduate'\n",
        "]\n",
        "# reindex to desired order\n",
        "class_vis = df['grade_category'].value_counts()\n",
        "class_vis = class_vis.reindex(class_order)\n",
        "\n",
        "# bar plot of grade levels\n",
        "plt.figure(figsize=(10, 6))\n",
        "ax = class_vis.plot(kind='bar', color='salmon', edgecolor='black')\n",
        "# total respondents equation\n",
        "plt.title(f'Grade Level Distribution', fontsize=16, fontweight='bold')\n",
        "plt.xlabel('Grade Level', fontsize=13, fontweight='bold')\n",
        "plt.ylabel('Number of Respondents', fontsize=13, fontweight='bold')\n",
        "# change background to improve aesthetics\n",
        "ax.set_facecolor('#f4f4f7')  # light gray for plot area\n",
        "plt.gcf().set_facecolor('#f9fafb')  # very light gray for overall figure\n",
        "# add dashed grid\n",
        "ax.grid(axis='y', linestyle='--', color='gray', linewidth=0.7, alpha=0.6)\n",
        "# add n-count on top of bars\n",
        "for i, v in enumerate(class_vis):\n",
        "    if pd.notna(v):\n",
        "        ax.text(i, v + 0.7, f'n = {int(v)}', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vfv0ZSE0TQ-X"
      },
      "outputs": [],
      "source": [
        "# soft pastel\n",
        "colors = ['#f7cac9', '#92b4ec', '#fff2b2', '#b6e2d3', '#f6eac2']\n",
        "\n",
        "# personal modifications\n",
        "grade_counts = df['grade_category'].value_counts().sort_index()\n",
        "labels = grade_counts.index\n",
        "sizes = grade_counts.values\n",
        "\n",
        "# pie chart of grade distribution\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.pie(\n",
        "    sizes,\n",
        "    labels=labels,\n",
        "    autopct='%1.1f%%',\n",
        "    startangle=90,\n",
        "    colors=colors,\n",
        "    wedgeprops={'edgecolor': 'white', 'linewidth': 2}\n",
        ")\n",
        "plt.title(f'Grade Level Distribution (n = {sum(sizes)})', fontsize=15, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pO9AVbvn1mS"
      },
      "source": [
        "**ENCODING GENDER**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nFbQ9dsQn4Ig"
      },
      "outputs": [],
      "source": [
        "gender_encoding = {\n",
        "    'Male': 1,\n",
        "    'Female': 2,\n",
        "    'Non-Binary': 3\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9uFTsHApn997"
      },
      "outputs": [],
      "source": [
        "# apply mapping to create encoded column\n",
        "df['gender_encoded'] = df['What gender do you identify as?'].map(gender_encoding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JIuHKcNqoMyB"
      },
      "outputs": [],
      "source": [
        "# print result\n",
        "print(df[['What gender do you identify as?', 'gender_encoded']])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I7b63zq_eKzQ"
      },
      "outputs": [],
      "source": [
        "# count values and sort alphabetically\n",
        "gender_counts = df['What gender do you identify as?'].value_counts().sort_index()\n",
        "labels = gender_counts.index\n",
        "values = gender_counts.values\n",
        "\n",
        "# bar plot for gender distrubution\n",
        "plt.figure(figsize=(8, 5))\n",
        "bars = plt.bar(labels, values, color='#6b4fbb', edgecolor='white', width=0.6)\n",
        "# add value labels above bar\n",
        "for bar, value in zip(bars, values):\n",
        "    plt.text(bar.get_x() + bar.get_width() / 2, value + 1, f'n = {value}',\n",
        "             ha='center', va='bottom', fontsize=11, fontweight='bold', color='#22223b')\n",
        "# subtle grey effect\n",
        "plt.gca().set_facecolor('#f6f7fa')\n",
        "plt.gcf().set_facecolor('#faf9f6')\n",
        "plt.grid(axis='y', linestyle='--', color='gray', linewidth=0.7, alpha=0.4)\n",
        "# titling\n",
        "total_n = int(values.sum())\n",
        "plt.title(f'Gender Distribution (n = {total_n})', fontsize=16, fontweight='bold', color='#22223b')\n",
        "plt.xlabel('Gender', fontsize=13, fontweight='bold')\n",
        "plt.ylabel('Number of Respondents', fontsize=13, fontweight='bold')\n",
        "plt.xticks(fontsize=12)\n",
        "plt.yticks(fontsize=12)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1UpIpwNJoXZa"
      },
      "source": [
        "**MAPPING AND ENCODING MAJOR**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ItAvR32oc0h"
      },
      "outputs": [],
      "source": [
        "# unique values of major\n",
        "df['What is your major?'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X6KsxwWFudQO"
      },
      "outputs": [],
      "source": [
        "category_keywords = {\n",
        "    'it': [\n",
        "        'computer science', 'cs', 'data science', 'applied math', 'computer engineering', 'informatics', 'cybersecurity'\n",
        "    ],\n",
        "    'engineering': [\n",
        "        'engineering', 'meche', 'mechanical engineering', 'civil engineering', 'chemical engineering',\n",
        "        'electrical engineering', 'biomedical engineering'\n",
        "    ],\n",
        "    'natural_sci': [\n",
        "        'biology', 'biological science', 'neuroscience', 'chemistry', 'biotech', 'medical lab science',\n",
        "        'animal science', 'plant science', 'marine biology', 'geological oceanography', 'wildlife', 'conservation',\n",
        "        'environmental', 'kinese', 'kinesiology'\n",
        "    ],\n",
        "    'social_sci': [\n",
        "        'political science', 'psychology', 'sociology', 'anthropology', 'public relations', 'ccj', 'soc', 'hdf'\n",
        "    ],\n",
        "    'business': [\n",
        "        'accounting', 'finance', 'business', 'marketing', 'economics'\n",
        "    ],\n",
        "    'arts': [\n",
        "        'film', 'media', 'literature', 'english', 'art', 'fine arts', 'music', 'theatre', 'directing'\n",
        "    ],\n",
        "    'education': [\n",
        "        'education', 'elementary', 'secondary', 'teaching', 'doctor of physical therapy', 'physical therapy'\n",
        "    ],\n",
        "    'other': [\n",
        "        'landscape architecture', 'marine affairs', 'textile merchandising', 'tmd', 'chinese', 'undecided'\n",
        "    ]\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VxBaqWicunuD"
      },
      "outputs": [],
      "source": [
        "# robust category function\n",
        "def categorize_major(response, keyword_map=category_keywords):\n",
        "    if not isinstance(response, str):\n",
        "        return 'other'\n",
        "    text = response.lower().strip()\n",
        "    for category, keywords in keyword_map.items():\n",
        "        for keyword in keywords:\n",
        "            if keyword in text:\n",
        "                return category\n",
        "    return 'other'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XrppHzz9urgc"
      },
      "outputs": [],
      "source": [
        "# apply mapping\n",
        "df['major_category'] = df['What is your major?'].apply(categorize_major)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QwrDe6ActRjP"
      },
      "outputs": [],
      "source": [
        "# numeric encoding\n",
        "category_codes = {cat: i for i, cat in enumerate(category_keywords.keys())}\n",
        "df['major_encoded'] = df['major_category'].map(category_codes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zRbOdofzILXA"
      },
      "outputs": [],
      "source": [
        "# print result\n",
        "print(df[['What is your major?', 'major_category','major_encoded']])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k2r31t2SP-AJ"
      },
      "outputs": [],
      "source": [
        "# count category responses\n",
        "category_counts = df['major_category'].value_counts()\n",
        "print(category_counts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i_8hDhaBQenC"
      },
      "outputs": [],
      "source": [
        "categories = ['it', 'engineering', 'natural_sci', 'social_sci', 'business', 'arts', 'education', 'other']\n",
        "counts = df['major_category'].value_counts().reindex(categories, fill_value=0)\n",
        "plt.figure(figsize=(9, 5))\n",
        "bars = plt.bar(categories, counts, color='#4f8fd1', edgecolor='black')\n",
        "\n",
        "# include number labels above bars\n",
        "for bar in bars:\n",
        "    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.5,\n",
        "             f\"{int(bar.get_height())}\", ha='center', va='bottom', fontsize=11, fontweight='bold', color='#2d2d2d')\n",
        "\n",
        "# plot style\n",
        "plt.gca().set_facecolor('#f3f3f3')\n",
        "plt.grid(axis='y', linestyle=':', color='gray', alpha=0.2)\n",
        "\n",
        "# title and axis labels\n",
        "plt.title('Number of Survey Responses by Major Category', fontsize=16, fontweight='bold')\n",
        "plt.xlabel('Major Category', fontsize=13, fontweight='bold')\n",
        "plt.ylabel('Number of Respondents', fontsize=13, fontweight='bold')\n",
        "\n",
        "plt.xticks(fontsize=11)\n",
        "plt.yticks(fontsize=11)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h8zbbOkYUyV3"
      },
      "outputs": [],
      "source": [
        "# soft pastel\n",
        "colors = ['#f7cac9', '#92b4ec', '#fff2b2', '#b6e2d3', '#f6eac2', '#c9c9ff', '#ffb3e6', '#b3ffb3'] # Added colors for all major categories\n",
        "\n",
        "# personal modifications\n",
        "grade_counts_major = df['major_category'].value_counts().sort_index()\n",
        "labels_major = grade_counts_major.index\n",
        "sizes_major = grade_counts_major.values\n",
        "\n",
        "# pie chart of major distribution\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.pie(\n",
        "    sizes_major,\n",
        "    labels=labels_major,\n",
        "    autopct='%1.1f%%',\n",
        "    startangle=90,\n",
        "    colors=colors,\n",
        "    wedgeprops={'edgecolor': 'white', 'linewidth': 2}\n",
        ")\n",
        "plt.title(f'Major Distribution', fontsize=15, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmd9CUzbwKKy"
      },
      "source": [
        "**MAPPING AND ENCODING ETHNICITY**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_2HJRZlRwPlr"
      },
      "outputs": [],
      "source": [
        "# unique values of major\n",
        "df['What is your race / ethnic background?'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7YxdJMvZwUvq"
      },
      "outputs": [],
      "source": [
        "# category keywords\n",
        "ethnicity_keywords = {\n",
        "    'asian': ['asian'],\n",
        "    'white': ['white', 'caucasian'],\n",
        "    'latino': ['latino', 'hispanic'],\n",
        "    'black': ['black', 'african american'],\n",
        "    'native_american': ['native american'],\n",
        "    'multiracial': ['mixed', 'multiracial', 'african american and native american'],\n",
        "    'notsay': ['prefer not to say'],\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8GdrwDJI0DV2"
      },
      "outputs": [],
      "source": [
        "# categorization function\n",
        "import re\n",
        "\n",
        "def categorize_ethnicity(response, keyword_map=ethnicity_keywords):\n",
        "    if not isinstance(response, str):\n",
        "        return 'other'\n",
        "    text = response.lower().strip()\n",
        "    for category, keywords in keyword_map.items():\n",
        "        for keyword in keywords:\n",
        "            # use regex for whole word (avoid 'asian' in 'caucasian')\n",
        "            if re.search(r'\\b' + re.escape(keyword) + r'\\b', text):\n",
        "                return category\n",
        "    return 'other'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kTccOxlG0X4o"
      },
      "outputs": [],
      "source": [
        "# apply to dataframe\n",
        "df['ethnicity_category'] = df['What is your race / ethnic background?'].apply(categorize_ethnicity)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lk9gRfX_0bL_"
      },
      "outputs": [],
      "source": [
        "# encode categories\n",
        "ethnicity_codes = {cat: i for i, cat in enumerate(ethnicity_keywords.keys())}\n",
        "df['ethnicity_encoded'] = df['ethnicity_category'].map(ethnicity_codes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bx_z0kfw3-J5"
      },
      "outputs": [],
      "source": [
        "# any missing values?\n",
        "df['What is your race / ethnic background?'].isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b8D6aLgB4XLr"
      },
      "outputs": [],
      "source": [
        "# print result\n",
        "print(df[['What is your race / ethnic background?', 'ethnicity_category', 'ethnicity_encoded']])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F8ONbh04e6hE"
      },
      "outputs": [],
      "source": [
        "# Count occurrences and ensure all categories are present\n",
        "categories = list(ethnicity_keywords.keys())\n",
        "counts = df['ethnicity_category'].value_counts().reindex(categories, fill_value=0)\n",
        "\n",
        "# ----- Bar Chart -----\n",
        "plt.figure(figsize=(8,5))\n",
        "bars = plt.bar(categories, counts, color='#63abd9', edgecolor='black')\n",
        "\n",
        "# Value labels on bars\n",
        "for bar in bars:\n",
        "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n",
        "             f\"{int(bar.get_height())}\", ha='center', va='bottom',\n",
        "             fontsize=11, fontweight='bold', color='#353535')\n",
        "\n",
        "plt.gca().set_facecolor('#f7f7fa')\n",
        "plt.grid(axis='y', linestyle=':', color='gray', alpha=0.25)\n",
        "plt.title('Survey Respondents by Ethnicity Category', fontsize=16, fontweight='bold')\n",
        "plt.xlabel('Ethnicity Category', fontsize=13, fontweight='bold')\n",
        "plt.ylabel('Number of Respondents', fontsize=13, fontweight='bold')\n",
        "plt.xticks(fontsize=11)\n",
        "plt.yticks(fontsize=11)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JWXcc7xOfAF3"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(7,7))\n",
        "colors = plt.get_cmap('tab20').colors\n",
        "patches, texts, autotexts = plt.pie(\n",
        "    counts, labels=categories, autopct='%1.1f%%', colors=colors[:len(categories)],\n",
        "    startangle=140, textprops={'fontsize': 12}, wedgeprops={'edgecolor': 'white'}\n",
        ")\n",
        "plt.setp(autotexts, size=13, weight='bold')\n",
        "plt.title('Ethnicity Proportion of Survey Respondents', fontsize=15, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOrkULUb5p9y"
      },
      "source": [
        "**MAPPING AND ENCODING AI TOOL FAMILIARITY**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BvsBGexC5yS6"
      },
      "outputs": [],
      "source": [
        "# unique values for ai tool familiarity\n",
        "df['How familiar are you Artificial Intelligence tools?'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "etCI6MV6Z_Rk"
      },
      "outputs": [],
      "source": [
        "familiarity_keywords = {\n",
        "    'Very Familiar': [\n",
        "        'very familiar', 'extremely familiar', 'absolutely', 'use it daily',\n",
        "        'conducted my own research', 'work on a grant', 'expert', 'avid',\n",
        "        'completely'\n",
        "    ],\n",
        "    'Quite Familiar': [\n",
        "        'fairly familiar', 'pretty familiar', 'moderately familiar',\n",
        "        'reasonably familiar', 'proficient', 'intermediate', 'regular use',\n",
        "        'comfortable'\n",
        "    ],\n",
        "    'Somewhat Familiar': [\n",
        "        'somewhat familiar', 'semi-familiar', 'semi familiar', 'alright',\n",
        "        'average', 'relatively familiar', 'kinda familiar', 'sort of',\n",
        "        'decent', 'have used',\n",
        "        'familiar with its use, not with its development',\n",
        "        'don’t use extensively', 'not as much as other students'\n",
        "    ],\n",
        "    'Slightly Familiar': [\n",
        "        'not very familiar', 'just a little', 'a little', 'basic', 'vague',\n",
        "        'only use chatgpt', 'aware', 'not really', 'only used a little',\n",
        "        'not much', 'not as much', 'slightly'\n",
        "    ]\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e76EwhLjcSNA"
      },
      "outputs": [],
      "source": [
        "# categorization function\n",
        "def categorize_familiarity(response):\n",
        "    if not isinstance(response, str):\n",
        "        return 'Slightly Familiar'\n",
        "    text = response.lower().strip()\n",
        "    # Priority: High Familiarity → Negative Familiarity → Neutral\n",
        "    for keyword in familiarity_keywords['Very Familiar']:\n",
        "        if keyword in text:\n",
        "            return 'Very Familiar'\n",
        "    for keyword in familiarity_keywords['Quite Familiar']:\n",
        "        if keyword in text:\n",
        "            return 'Quite Familiar'\n",
        "    for keyword in familiarity_keywords['Somewhat Familiar']:\n",
        "        if keyword in text:\n",
        "            return 'Somewhat Familiar'\n",
        "    for keyword in familiarity_keywords['Slightly Familiar']:\n",
        "        if keyword in text:\n",
        "            return 'Slightly Familiar'\n",
        "    # Fallbacks based on presence of \"familiar\"\n",
        "    if 'familiar' in text:\n",
        "        return 'Somewhat Familiar'\n",
        "    return 'Slightly Familiar'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1xftR47ecdy3"
      },
      "outputs": [],
      "source": [
        "# category assignment\n",
        "df['familiarity_category'] = df['How familiar are you Artificial Intelligence tools?'].apply(categorize_familiarity)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E8NDF62Ickce"
      },
      "outputs": [],
      "source": [
        "# map ordinal categories\n",
        "category_order = ['Slightly Familiar', 'Somewhat Familiar', 'Quite Familiar', 'Very Familiar']\n",
        "category_codes = {cat: i for i, cat in enumerate(category_order)}\n",
        "df['familiarity_encoded'] = df['familiarity_category'].map(category_codes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ULQh-LBAeugf"
      },
      "outputs": [],
      "source": [
        "# any missing values\n",
        "df['How familiar are you Artificial Intelligence tools?'].isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1zZAF1WkcpCj"
      },
      "outputs": [],
      "source": [
        "# bar chart\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "counts = df['familiarity_category'].value_counts().reindex(category_order, fill_value=0)\n",
        "plt.figure(figsize=(8,5))\n",
        "bars = plt.bar(category_order, counts, color='#7dd8be', edgecolor='black')\n",
        "for bar in bars:\n",
        "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height()+0.5, int(bar.get_height()),\n",
        "             ha='center', fontweight='bold', fontsize=13)\n",
        "plt.title(\"AI Familiarity Among Respondents\", fontweight='bold', fontsize=16)\n",
        "plt.xlabel(\"Familiarity Level\", fontweight='bold', fontsize=13)\n",
        "plt.ylabel(\"Number of Respondents\", fontweight='bold', fontsize=13)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5SeA8g5kheba"
      },
      "outputs": [],
      "source": [
        "# pie chart\n",
        "colors = ['#ff9999','#66b3ff','#99ff99','#ffcc99']\n",
        "\n",
        "plt.figure(figsize=(7,7))\n",
        "patches, texts, autotexts = plt.pie(\n",
        "    counts,\n",
        "    labels=category_order,\n",
        "    autopct='%1.1f%%',\n",
        "    colors=colors,\n",
        "    startangle=140,\n",
        "    textprops={'fontsize': 12},\n",
        "    wedgeprops={'edgecolor': 'white'}\n",
        ")\n",
        "\n",
        "plt.setp(autotexts, size=13, weight='bold')\n",
        "plt.title('Respondent Familiarity with AI Tools', fontsize=16, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JpRUcN5i-wzr"
      },
      "source": [
        "**DURATION OF AI TOOL USE MAPPING AND ENCODING**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yD9usVxGo1-9"
      },
      "outputs": [],
      "source": [
        "encoding_map = {\n",
        "    'Within the last year': 1,\n",
        "    '1-2 years': 2,\n",
        "    '2-3 years': 3,\n",
        "    '3-4 years': 4,\n",
        "    '4+ years': 5\n",
        "}\n",
        "\n",
        "df['duration_encoded'] = df['How long have you been using AI tools for?'].map(encoding_map)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2edrAheeo2gQ"
      },
      "outputs": [],
      "source": [
        "# define categories\n",
        "categories = [\n",
        "    'Within the last year',\n",
        "    '1-2 years',\n",
        "    '2-3 years',\n",
        "    '3-4 years',\n",
        "    '4+ years'\n",
        "]\n",
        "counts = df['How long have you been using AI tools for?'].value_counts().reindex(categories, fill_value=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oqqXPUXUo_j_"
      },
      "outputs": [],
      "source": [
        "# bar plot\n",
        "plt.figure(figsize=(9, 6))\n",
        "bars = plt.bar(categories, counts, color='#6db3f7', edgecolor='black')\n",
        "for bar in bars:\n",
        "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height()+0.5,\n",
        "             int(bar.get_height()), ha='center', fontweight='bold', fontsize=13)\n",
        "\n",
        "plt.title('Duration of AI Tool Use Among Respondents', fontsize=16, fontweight='bold')\n",
        "plt.xlabel('AI Tool Use Duration', fontsize=13, fontweight='bold')\n",
        "plt.ylabel('Number of Respondents', fontsize=13, fontweight='bold')\n",
        "plt.xticks(rotation=20, fontsize=11)\n",
        "plt.yticks(fontsize=11)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F-vtrVBnpEAG"
      },
      "outputs": [],
      "source": [
        "# pie chart\n",
        "colors = ['#8dd3c7', '#80b1d3', '#fdb462', '#b3de69', '#fb8072']\n",
        "\n",
        "plt.figure(figsize=(7,7))\n",
        "patches, texts, autotexts = plt.pie(\n",
        "    counts,\n",
        "    labels=categories,\n",
        "    autopct='%1.1f%%',\n",
        "    colors=colors,\n",
        "    startangle=140,\n",
        "    textprops={'fontsize': 12},\n",
        "    wedgeprops={'edgecolor': 'white'}\n",
        ")\n",
        "plt.setp(autotexts, size=13, weight='bold')\n",
        "plt.title('Respondent Distribution by AI Tool Use Duration', fontsize=16, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DO3WZQUS_YA9"
      },
      "outputs": [],
      "source": [
        "# how many missing values\n",
        "df['How long have you been using AI tools for?'].isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SvHtGMIz_nxY"
      },
      "outputs": [],
      "source": [
        "# print result\n",
        "print(df[['How long have you been using AI tools for?', 'duration_encoded']])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wOF6JSav_rsB"
      },
      "source": [
        "**LIST TO AI TOOLS BEING USED**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TrYCBUP0K46-"
      },
      "outputs": [],
      "source": [
        "# unique values of ai tools being used\n",
        "df['What Artificial Intelligence tools do you frequently use? (If you use any tools in mind that are not in the checklist, please include in the \"Other...\" section)'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ipK7ix37wZn_"
      },
      "outputs": [],
      "source": [
        "# split and clean student response\n",
        "def split_tools(response):\n",
        "    if not isinstance(response, str) or response.strip() == '':\n",
        "        return []\n",
        "    return [tool.strip().lower() for tool in response.split(',')]\n",
        "\n",
        "df['ai_tools_list'] = df['What Artificial Intelligence tools do you frequently use? (If you use any tools in mind that are not in the checklist, please include in the \"Other...\" section)'].apply(split_tools)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RRX9_V2hwwjO"
      },
      "outputs": [],
      "source": [
        "# explode list for frequency analysis\n",
        "df_exploded = df.explode('ai_tools_list')\n",
        "df_exploded = df_exploded[df_exploded['ai_tools_list'].notna() & (df_exploded['ai_tools_list'] != '')]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZSF-W5rXw0uM"
      },
      "outputs": [],
      "source": [
        "# calculate frequency\n",
        "tool_counts = df_exploded['ai_tools_list'].value_counts().sort_values(ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XliBRjx64uHA"
      },
      "outputs": [],
      "source": [
        "# filter for clarity\n",
        "tool_counts_filtered = tool_counts[tool_counts >= 5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "97da9m1Mw4M3"
      },
      "outputs": [],
      "source": [
        "# bar chart visualization\n",
        "plt.figure(figsize=(8, 5))\n",
        "bars = plt.bar(tool_counts_filtered.index, tool_counts_filtered.values, color='#6db3f7', edgecolor='black')\n",
        "plt.title('Most Common AI Tools Used (5+ Responses)', fontsize=15, fontweight='bold')\n",
        "plt.xlabel('AI Tool', fontsize=12, fontweight='bold')\n",
        "plt.ylabel('Number of Users', fontsize=12, fontweight='bold')\n",
        "plt.xticks(rotation=25, ha='right', fontsize=11)\n",
        "for bar in bars:\n",
        "    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.5, int(bar.get_height()), ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zlQiWoo6w8wI"
      },
      "outputs": [],
      "source": [
        "# pie chart\n",
        "plt.figure(figsize=(7, 7))\n",
        "plt.pie(\n",
        "    tool_counts_filtered.values,\n",
        "    labels=tool_counts_filtered.index,\n",
        "    autopct='%1.1f%%',\n",
        "    startangle=140,\n",
        "    textprops={'fontsize': 12, 'weight': 'bold'},\n",
        "    wedgeprops={'edgecolor': 'white'}\n",
        ")\n",
        "plt.title('Proportion of AI Tool Usage (5+ Responses)', fontsize=15, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# demonstrating encoding\n",
        "print(df[['What Artificial Intelligence tools do you frequently use? (If you use any tools in mind that are not in the checklist, please include in the \"Other...\" section)', 'ai_tools_list']])"
      ],
      "metadata": {
        "id": "uDr1sAGOp-3K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-09hwmKE4-Sf"
      },
      "outputs": [],
      "source": [
        "# any missing values?\n",
        "df['What Artificial Intelligence tools do you frequently use? (If you use any tools in mind that are not in the checklist, please include in the \"Other...\" section)'].isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FeGYN-eusZuH"
      },
      "source": [
        "**ENCODING AND CLEANING EXPLANATION OF FINITE AMOUNT OF AI USE**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w2_0hvPSsfr7"
      },
      "outputs": [],
      "source": [
        "# unique values of finite ai use\n",
        "df['How often do you use these AI tools? Please estimate how many times per week or month you use these tools.'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gt9CltpR6RCL"
      },
      "outputs": [],
      "source": [
        "# keywords\n",
        "frequency_keywords = {\n",
        "    'Never or Rarely': [\n",
        "        'rarely', 'not really ever', 'never', 'i try not to', 'not often', 'no generative', 'avoid', 'haven\\'t had to use it', 'not often anymore'\n",
        "    ],\n",
        "    'Monthly or Less': [\n",
        "        'once a month', 'once per month', 'twice a month', 'every month', 'once maybe', 'once every month', 'every other month', 'less than once a month', '1-2 times per month', 'once or twice a month', 'maybe once a month', 'about three times a month', 'maybe once every two weeks', 'once every two weeks', 'twice a month', 'monthly'\n",
        "    ],\n",
        "    'Weekly': [\n",
        "        'per week', 'weekly', 'a week', 'couple times a week', 'once a week', 'twice a week', '2-3 times a week', '3-4 times a week', 'few times a week', 'several times a week', 'maybe four times a week', '1-3 times a week', '2-5 times per week', '1-2 times per week', 'maybe 3 times a week', 'at least once a week', 'two to three times a week', 'four times a week', 'five times a week', 'several times a week', 'every week', 'few times a week', 'during school, at least once a week'\n",
        "    ],\n",
        "    'Daily or More': [\n",
        "        'every day', 'everyday', 'daily', 'once per day', 'once a day', 'multiple times per day', 'several times daily', 'all the time', 'twice a day', 'five times a day', '25/8'\n",
        "    ]\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2RCQYmeO6UPd"
      },
      "outputs": [],
      "source": [
        "# categorization function\n",
        "def categorize_frequency(response, keyword_map=frequency_keywords):\n",
        "    if not isinstance(response, str) or response.strip() == \"\":\n",
        "        return \"Never or Rarely\"\n",
        "    text = response.lower().strip()\n",
        "    # specific/high frequency bins\n",
        "    for category in ['Daily or More', 'Weekly', 'Monthly or Less', 'Never or Rarely']:\n",
        "        for keyword in keyword_map[category]:\n",
        "            if keyword in text:\n",
        "                return category\n",
        "    # numerical clues\n",
        "    if any(n in text for n in ['x per week', 'times a week', 'per week', 'a week']):\n",
        "        return \"Weekly\"\n",
        "    if any(n in text for n in ['per day', 'a day', 'every day', 'daily']):\n",
        "        return \"Daily or More\"\n",
        "    if any(n in text for n in ['per month', 'a month', 'every month']):\n",
        "        return \"Monthly or Less\"\n",
        "    # final fallback\n",
        "    return \"Never or Rarely\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-TERzdDF6blv"
      },
      "outputs": [],
      "source": [
        "# apply dataframe\n",
        "df['frequency_category'] = df['How often do you use these AI tools? Please estimate how many times per week or month you use these tools.'].apply(categorize_frequency)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8sSALknv6h2q"
      },
      "outputs": [],
      "source": [
        "# numeric encoding\n",
        "category_order = ['Never or Rarely', 'Monthly or Less', 'Weekly', 'Daily or More']\n",
        "category_codes = {cat: i for i, cat in enumerate(category_order)}\n",
        "df['frequency_encoded'] = df['frequency_category'].map(category_codes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uPNZLG74kdH4"
      },
      "outputs": [],
      "source": [
        "# bar chart\n",
        "counts = df['frequency_category'].value_counts().reindex(category_order, fill_value=0)\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "bars = plt.bar(category_order, counts, color='#7daef7', edgecolor='black')\n",
        "for bar in bars:\n",
        "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height()+0.5,\n",
        "             int(bar.get_height()), ha='center', fontweight='bold', fontsize=13)\n",
        "plt.title('Frequency of AI Tool Use', fontweight='bold', fontsize=16)\n",
        "plt.xlabel('Usage Frequency', fontweight='bold', fontsize=13)\n",
        "plt.ylabel('Number of Respondents', fontweight='bold', fontsize=13)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Y8-zfzJlGmE"
      },
      "outputs": [],
      "source": [
        "# pie chart\n",
        "plt.figure(figsize=(7,7))\n",
        "colors = ['#bdbdbd', '#90ee90', '#89cff0', '#ffa07a']\n",
        "plt.pie(counts, labels=category_order, autopct='%1.1f%%', startangle=140,\n",
        "        colors=colors, textprops={'fontsize':12, 'weight':'bold'},\n",
        "        wedgeprops={'edgecolor':'white'})\n",
        "plt.title('Distribution of AI Tool Use Frequency', fontsize=16, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# demonstrating encoding\n",
        "print(df[['How often do you use these AI tools? Please estimate how many times per week or month you use these tools.', 'frequency_category', 'frequency_encoded']])"
      ],
      "metadata": {
        "id": "q3r8EKmzpz3U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8l_29lxsYWY"
      },
      "source": [
        "**MAPPING AND ENCODING TYPES OF ACADEMIC TASKS**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KkYioUia4xt4"
      },
      "outputs": [],
      "source": [
        "df['What types of academic tasks do you use AI tools for?'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M3f5cOSlAZHw"
      },
      "outputs": [],
      "source": [
        "# split and clean response\n",
        "def split_tasks(response):\n",
        "    if not isinstance(response, str) or response.strip() == '':\n",
        "        return []\n",
        "    # Lowercase for uniformity and strip whitespace\n",
        "    return [task.strip().capitalize() for task in response.split(',') if task.strip()]\n",
        "\n",
        "df['ai_tasks_list'] = df['What types of academic tasks do you use AI tools for?'].apply(split_tasks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AO09ZA-xAlRE"
      },
      "outputs": [],
      "source": [
        "# explode list\n",
        "df_exploded = df.explode('ai_tasks_list')\n",
        "# count task use\n",
        "df_exploded = df_exploded[df_exploded['ai_tasks_list'].notna() & (df_exploded['ai_tasks_list'] != '')]\n",
        "task_counts = df_exploded['ai_tasks_list'].value_counts().sort_values(ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6e9wk3JmAtaK"
      },
      "outputs": [],
      "source": [
        "# filter for clarity\n",
        "filtered_task_counts = task_counts[task_counts >= 5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DLkgPNCBAzgc"
      },
      "outputs": [],
      "source": [
        "# bar chart\n",
        "plt.figure(figsize=(8, 5))\n",
        "bars = plt.bar(filtered_task_counts.index, filtered_task_counts.values, color='#6db3f7', edgecolor='black')\n",
        "plt.title('AI Tasks Usage - Bar Chart', fontsize=16, fontweight='bold')\n",
        "plt.xlabel('Academic Tasks', fontsize=14, fontweight='bold')\n",
        "plt.ylabel('Number of Students', fontsize=14, fontweight='bold')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "for bar in bars:\n",
        "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, int(bar.get_height()), ha='center', fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eKfYvT8_A20A"
      },
      "outputs": [],
      "source": [
        "# pie chart\n",
        "plt.figure(figsize=(6, 6))\n",
        "colors = plt.get_cmap('tab20').colors[:len(filtered_task_counts)]\n",
        "patches, texts, autotexts = plt.pie(\n",
        "    filtered_task_counts.values,\n",
        "    labels=filtered_task_counts.index,\n",
        "    autopct='%1.1f%%',\n",
        "    startangle=140,\n",
        "    colors=colors,\n",
        "    textprops={'fontsize': 12},\n",
        "    wedgeprops={'edgecolor': 'white'}\n",
        ")\n",
        "plt.setp(autotexts, size=13, weight='bold')\n",
        "plt.title('AI Tasks Usage - Pie Chart', fontsize=16, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vS1_ie7S9BCX"
      },
      "source": [
        "**MAPPING AND ENCODING COLLEGE SUBJECTS OF AI USE**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6BExC70q_KJZ"
      },
      "outputs": [],
      "source": [
        "# unique values of college subjects\n",
        "df['What college subjects do you often use AI tools for? Please list specific course names or subject areas.'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "erXap8duBeqO"
      },
      "outputs": [],
      "source": [
        "# split and clean data\n",
        "def split_tasks(response):\n",
        "    if not isinstance(response, str) or response.strip() == '':\n",
        "        return []\n",
        "    return [task.strip() for task in response.split(',') if task.strip()]\n",
        "\n",
        "df['ai_subjects_list'] = df['What college subjects do you often use AI tools for? Please list specific course names or subject areas.'].apply(split_tasks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "psWSYobqFfYm"
      },
      "outputs": [],
      "source": [
        "# explode list for frequency analysis\n",
        "df_exploded = df.explode('ai_subjects_list')\n",
        "# column uniformity\n",
        "df_exploded['ai_subjects_list'] = df_exploded['ai_subjects_list'].astype(str).str.strip().str.lower()\n",
        "# remove  parentheses and content\n",
        "def strip_parentheses(text):\n",
        "    return re.sub(r'\\s*\\(.*?\\)\\s*', '', text).strip()\n",
        "\n",
        "df_exploded['ai_subjects_list'] = df_exploded['ai_subjects_list'].apply(strip_parentheses)\n",
        "\n",
        "# remove 'etc.' variants\n",
        "df_exploded = df_exploded[~df_exploded['ai_subjects_list'].str.fullmatch(r'etc\\.?', case=False, na=False)]\n",
        "\n",
        "# remove any residual empty or blank categories\n",
        "df_exploded = df_exploded[df_exploded['ai_subjects_list'].str.strip() != '']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BrpyCV_rFnZv"
      },
      "outputs": [],
      "source": [
        "# count task frequency\n",
        "task_counts = df_exploded['ai_subjects_list'].value_counts()\n",
        "task_counts_filtered = task_counts[task_counts >= 20]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qeHEjsLtFrA8"
      },
      "outputs": [],
      "source": [
        "# bar chart\n",
        "plt.figure(figsize=(8, 5))\n",
        "bars = plt.bar(task_counts_filtered.index, task_counts_filtered.values,\n",
        "               color='#6db3f7', edgecolor='black')\n",
        "plt.title('Most Common Academic Subjects for AI Tools', fontsize=16, fontweight='bold')\n",
        "plt.xlabel('Task', fontsize=13, fontweight='bold')\n",
        "plt.ylabel('Number of Students', fontsize=13, fontweight='bold')\n",
        "plt.xticks(rotation=25, ha='right')\n",
        "for bar in bars:\n",
        "    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.5,\n",
        "             int(bar.get_height()), ha='center', fontweight='bold', fontsize=11)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I5HgH9flFvMK"
      },
      "outputs": [],
      "source": [
        "# pie chart\n",
        "plt.figure(figsize=(7, 7))\n",
        "colors = plt.get_cmap('tab20').colors[:len(task_counts_filtered)]\n",
        "patches, texts, autotexts = plt.pie(\n",
        "    task_counts_filtered.values,\n",
        "    labels=task_counts_filtered.index,\n",
        "    autopct='%1.1f%%',\n",
        "    startangle=140,\n",
        "    colors=colors,\n",
        "    textprops={'fontsize': 12, 'weight': 'bold'},\n",
        "    wedgeprops={'edgecolor': 'white'}\n",
        ")\n",
        "plt.title('Proportion of AI Task Usage (Common Tasks)', fontsize=16, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IkrtcNYJdD9v"
      },
      "source": [
        "**ENCODING BINARY BENEFITS AND CHALLENGES**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_E5rAmVtd369"
      },
      "outputs": [],
      "source": [
        "# unique values\n",
        "df['Have you experienced any benefits or challenges when using AI tools for academic purposes?'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jChriGHEeA4X"
      },
      "outputs": [],
      "source": [
        "# encoding decision\n",
        "benchal_encoding = {\n",
        "    'Yes': 1,\n",
        "    'No': 2,\n",
        "}\n",
        "\n",
        "df['benchal_encoded'] = df['Have you experienced any benefits or challenges when using AI tools for academic purposes?'].map(benchal_encoding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2KsKK7iIGoYe"
      },
      "outputs": [],
      "source": [
        "# any missing data\n",
        "missing_count = df['benchal_encoded'].isnull().sum()\n",
        "print(f\"Missing or unmapped entries: {missing_count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NowcB87mGqho"
      },
      "outputs": [],
      "source": [
        "# bar chart\n",
        "benchal_categories = [\n",
        "    'Yes',\n",
        "    'No'\n",
        "]\n",
        "counts = df['Have you experienced any benefits or challenges when using AI tools for academic purposes?'].value_counts().reindex(benchal_categories, fill_value=0)\n",
        "plt.figure(figsize=(7, 5))\n",
        "bars = plt.bar(benchal_categories, counts, color='#6db3f7', edgecolor='black')\n",
        "for bar in bars:\n",
        "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, int(bar.get_height()),\n",
        "             ha='center', fontweight='bold', fontsize=12)\n",
        "plt.title('Distribution of Categories', fontsize=16, fontweight='bold')\n",
        "plt.xlabel('Categories', fontsize=14, fontweight='bold')\n",
        "plt.ylabel('Number of Respondents', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5CKPgnUsGwrD"
      },
      "outputs": [],
      "source": [
        "# pie chart\n",
        "colors = ['#ff9999','#66b3ff']\n",
        "\n",
        "plt.figure(figsize=(5, 5))\n",
        "patches, texts, autotexts = plt.pie(\n",
        "    counts,\n",
        "    labels=counts.index,  # Use the index of the counts Series as labels\n",
        "    autopct='%1.1f%%',\n",
        "    colors=colors,\n",
        "    startangle=140,\n",
        "    textprops={'fontsize': 12, 'weight': 'bold'},\n",
        "    wedgeprops={'edgecolor': 'white'}\n",
        ")\n",
        "\n",
        "plt.setp(autotexts, size=13, weight='bold')\n",
        "plt.title('Category Proportion', fontsize=16, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OcaAZZSZeekj"
      },
      "source": [
        "**ENCODE OPINION ON AFFECTING LEARNING APPROACH**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YetCP02AenET"
      },
      "outputs": [],
      "source": [
        "# unique values\n",
        "df['To what degree do you agree with the following statement?\\nAI tools have affected the way you approach learning or complete assignments.'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YQ-37JkzezL6"
      },
      "outputs": [],
      "source": [
        "# encode first opinion\n",
        "learning_encoded = {\n",
        "    'Strongly Agree': 5,\n",
        "    'Somewhat Agree': 4,\n",
        "    'Neutral': 3,\n",
        "    'Somewhat Disagree': 2,\n",
        "    'Strongly Disagree': 1\n",
        "}\n",
        "\n",
        "df['opinion_learning_encoded'] = df['To what degree do you agree with the following statement?\\nAI tools have affected the way you approach learning or complete assignments.'].map(learning_encoded)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Ds_cDJsIUvR"
      },
      "outputs": [],
      "source": [
        "# check missing values\n",
        "missing = df['opinion_learning_encoded'].isnull().sum()\n",
        "print(f'Missing/unmapped responses: {missing}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fAoz5F0rIecB"
      },
      "outputs": [],
      "source": [
        "# compute ordinal data\n",
        "learning_order = ['Strongly Disagree', 'Somewhat Disagree', 'Neutral', 'Somewhat Agree', 'Strongly Agree']\n",
        "counts = df['To what degree do you agree with the following statement?\\nAI tools have affected the way you approach learning or complete assignments.'].value_counts().reindex(learning_order, fill_value=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hEAD6u7EIpBV"
      },
      "outputs": [],
      "source": [
        "# bar chart\n",
        "plt.figure(figsize=(8,5))\n",
        "bars = plt.bar(learning_order, counts, color='#7daef7', edgecolor='black')\n",
        "\n",
        "for bar in bars:\n",
        "    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.3,\n",
        "             int(bar.get_height()), ha='center', fontweight='bold', fontsize=11)\n",
        "\n",
        "plt.title('Distribution of Agreement: AI Tools Affecting Learning Approach', fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Degree of Agreement', fontsize=12, fontweight='bold')\n",
        "plt.ylabel('Number of Respondents', fontsize=12, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i7aaujC5IsAl"
      },
      "outputs": [],
      "source": [
        "# pie chart\n",
        "colors = ['#ffb3b3','#ffd480','#ffffb3','#b3e6b3','#b3d1ff']  # One for each Likert category\n",
        "plt.figure(figsize=(7,7))\n",
        "plt.pie(\n",
        "    counts,\n",
        "    labels=learning_order,\n",
        "    autopct='%1.1f%%',\n",
        "    colors=colors,\n",
        "    startangle=140,\n",
        "    textprops={'fontsize':13, 'weight':'bold'},\n",
        "    wedgeprops={'edgecolor':'white'}\n",
        ")\n",
        "plt.title('Agreement with AI Tools Affecting Learning (Proportion)', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohDl4i7vfGEV"
      },
      "source": [
        "**ENCODING OPINION ON UNDERSTADNING COURSE MATERIAL**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rjxjkJ02fMMc"
      },
      "outputs": [],
      "source": [
        "# unique values\n",
        "df['To what degree do you agree with the following statement? \\nAI tools affected your ability to understand course material and learn independently.'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UjJhJUs-fXVQ"
      },
      "outputs": [],
      "source": [
        "# encode second opinion\n",
        "understand_encoding = {\n",
        "    'Strongly Agree': 5,\n",
        "    'Somewhat Agree': 4,\n",
        "    'Neutral': 3,\n",
        "    'Somewhat Disagree': 2,\n",
        "    'Strongly Disagree': 1\n",
        "}\n",
        "\n",
        "df['understand_encoding'] = df['To what degree do you agree with the following statement? \\nAI tools affected your ability to understand course material and learn independently.'].map(understand_encoding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GYtk1pfCJgW1"
      },
      "outputs": [],
      "source": [
        "# check for missing values\n",
        "missing = df['understand_encoding'].isnull().sum()\n",
        "print(f\"Missing/unmapped responses: {missing}\")\n",
        "if missing > 0:\n",
        "    print(df[df['understand_encoding'].isnull()][\n",
        "        'To what degree do you agree with the following statement?\\nAI tools affected your ability to understand course material and understand course assignments.'\n",
        "    ].unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ji1r_O0JJlWg"
      },
      "outputs": [],
      "source": [
        "# set and reindex ordinal data\n",
        "degree_understanding_order = [\n",
        "    'Strongly Disagree', 'Somewhat Disagree', 'Neutral', 'Somewhat Agree', 'Strongly Agree'\n",
        "]\n",
        "\n",
        "understanding_vis = df['To what degree do you agree with the following statement? \\nAI tools affected your ability to understand course material and learn independently.'].value_counts().reindex(degree_understanding_order, fill_value=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W78Xdp8nJwFD"
      },
      "outputs": [],
      "source": [
        "# bar chart\n",
        "plt.figure(figsize=(8,5))\n",
        "bars = plt.bar(understanding_vis.index, understanding_vis.values, color='#7daef7', edgecolor='black')\n",
        "for bar in bars:\n",
        "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height()+0.3, int(bar.get_height()), ha='center', fontweight='bold', fontsize=11)\n",
        "\n",
        "plt.title('Understanding Course Material Distribution', fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Degree of Agreement', fontsize=12, fontweight='bold')\n",
        "plt.ylabel('Number of Respondents', fontsize=12, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u1aNpfdbJyqX"
      },
      "outputs": [],
      "source": [
        "# pie chart\n",
        "colors = ['#ffb3b3','#ffd480','#ffffb3','#b3e6b3','#b3d1ff']\n",
        "plt.figure(figsize=(7,7))\n",
        "plt.pie(\n",
        "    understanding_vis.values,\n",
        "    labels=degree_understanding_order,\n",
        "    autopct='%1.1f%%',\n",
        "    colors=colors,\n",
        "    startangle=140,\n",
        "    textprops={'fontsize':13, 'weight':'bold'},\n",
        "    wedgeprops={'edgecolor':'white'}\n",
        ")\n",
        "plt.title('Understanding Course Material Distribution', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1PN0P4SwPTt"
      },
      "source": [
        "These are all close ended responses, though it is interesting how many different ways these needed to be encoded. For example, questions that allowed students to select more than one option need a different encoding method to accurate reflect the survey data, while making it easy for Python to interpret."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFl7yoxvfzl6"
      },
      "source": [
        "# OPEN ENDED PREPROCESSING"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLHWJedyg43u"
      },
      "source": [
        "Now that all the close ended questions are handled, more complex methods will be integrated to encode open ended responses."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3FOJvJCQGK9Y"
      },
      "source": [
        "**RECEIVING PROFESSOR INSTRUCTION ON USING AI IN SCHOOL**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-4K-sF3cGKPy"
      },
      "outputs": [],
      "source": [
        "# unique professor instruction values\n",
        "df['Have you ever received instruction from professors or URI itself about using AI tools for your coursework? If so, please describe the advice or policies you have encountered.'].unique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LAhaHl6xKffJ"
      },
      "source": [
        "This basic text cleaning will standardize the responses so they are more interpretable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zKLbStpbWfOk"
      },
      "outputs": [],
      "source": [
        "# necessary imports\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wu5WpR22Wl4t"
      },
      "outputs": [],
      "source": [
        "# prepare stopwords\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "custom_stopwords = {\n",
        "    # Generic English stopwords (expandable)\n",
        "    \"i\", \"me\", \"my\", \"we\", \"us\", \"our\", \"you\", \"your\", \"they\", \"them\", \"their\",\n",
        "    \"he\", \"she\", \"it\", \"its\", \"is\", \"was\", \"are\", \"were\", \"be\", \"been\", \"being\",\n",
        "    \"am\", \"do\", \"does\", \"did\", \"have\", \"has\", \"had\", \"can\", \"could\", \"would\",\n",
        "    \"should\", \"will\", \"shall\", \"may\", \"might\", \"must\", \"not\", \"no\", \"yes\", \"a\",\n",
        "    \"an\", \"the\", \"and\", \"or\", \"but\", \"so\", \"if\", \"then\", \"that\", \"this\", \"these\",\n",
        "    \"those\", \"of\", \"to\", \"in\", \"on\", \"at\", \"for\", \"with\", \"as\", \"by\", \"from\",\n",
        "    \"about\", \"into\", \"out\", \"up\", \"down\", \"over\", \"under\", \"again\", \"further\",\n",
        "    \"than\", \"too\", \"very\", \"more\", \"most\", \"some\", \"such\", \"only\", \"just\", \"also\",\n",
        "    \"even\", \"though\", \"although\", \"still\", \"while\", \"when\", \"where\", \"which\",\n",
        "    \"who\", \"whom\", \"whose\", \"how\", \"what\", \"why\",\n",
        "\n",
        "    # Common but irrelevant words for your context\n",
        "    \"class\", \"classes\", \"course\", \"courses\", \"professor\", \"professors\", \"teacher\",\n",
        "    \"teachers\", \"assignment\", \"assignments\", \"semester\", \"school\", \"student\", \"students\",\n",
        "    \"work\", \"used\", \"use\", \"using\", \"allows\", \"allowed\", \"allow\", \"told\", \"tells\",\n",
        "    \"said\", \"says\", \"taught\", \"tells\", \"told\", \"took\", \"taking\", \"help\", \"helps\", \"helping\",\n",
        "    \"tool\", \"tools\", \"one\", \"two\", \"three\", \"four\", \"five\", \"lot\", \"many\", \"much\",\n",
        "    \"thing\", \"things\", \"something\", \"nothing\", \"anything\", \"everything\",\n",
        "\n",
        "    # Repetitive or vague survey language\n",
        "    \"ai\", \"chatgpt\", \"copilot\", \"gpt\", \"llms\", \"large\", \"language\", \"model\",\n",
        "    \"etc\", \"example\", \"ex\", \"etc.\", \"thing\", \"things\", \"stuff\",\n",
        "    \"kind\", \"type\", \"types\", \"way\", \"ways\",\n",
        "\n",
        "    # Fillers, hedging\n",
        "    \"probably\", \"maybe\", \"sort\", \"kind\", \"basically\", \"generally\", \"usually\",\n",
        "    \"like\", \"really\", \"get\", \"getting\", \"got\", \"make\", \"making\", \"made\",\n",
        "\n",
        "    # Time/Location references that don’t help\n",
        "    \"last\", \"past\", \"now\", \"then\", \"before\", \"after\", \"currently\", \"recently\",\n",
        "    \"recent\", \"year\", \"years\", \"semester\", \"term\", \"month\", \"week\", \"day\",\n",
        "    \"uconn\", \"uri\", \"csc\",\n",
        "\n",
        "    # Other context-specific fluff\n",
        "    \"policy\", \"policies\", \"instruction\", \"instructed\", \"statement\", \"statements\",\n",
        "    \"response\", \"responses\", \"survey\", \"question\", \"answer\", \"tutor\", \"resource\",\n",
        "    \"resources\", \"feedback\", \"reference\", \"references\", \"cite\", \"cited\", \"citing\",\n",
        "    \"output\", \"prompt\", \"prompts\", \"detected\", \"detectors\"\n",
        "}\n",
        "stop_words.update(custom_stopwords)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uyVo3BSgoo7w"
      },
      "outputs": [],
      "source": [
        "# apply stemming\n",
        "from nltk.stem import PorterStemmer\n",
        "stemmer = PorterStemmer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3jUTnErEWt2y"
      },
      "outputs": [],
      "source": [
        "# define cleaning function\n",
        "def ai_prof_preprocess(text):\n",
        "    if not isinstance(text, str):\n",
        "        return ''\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "    tokens = text.split()\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "    return ' '.join(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mtoEZUBobsNL"
      },
      "outputs": [],
      "source": [
        "# apply cleaning\n",
        "df['ai_instr_clean'] = df['Have you ever received instruction from professors or URI itself about using AI tools for your coursework? If so, please describe the advice or policies you have encountered.'].apply(ai_prof_preprocess)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rTH3Qftnbwu_"
      },
      "outputs": [],
      "source": [
        "# remove extra spaces\n",
        "df['ai_instr_clean'] = df['ai_instr_clean'].str.replace(r'\\s+', ' ', regex=True).str.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y6mPNGlMb0OJ"
      },
      "outputs": [],
      "source": [
        "# spot check\n",
        "print(df[['Have you ever received instruction from professors or URI itself about using AI tools for your coursework? If so, please describe the advice or policies you have encountered.', 'ai_instr_clean']].head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rzcwg5eedkU1"
      },
      "source": [
        "This is the open-ended method of choice for now. Below will be a couple of related visualizations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_arIKOX3do6P"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# flatten all words into a single list\n",
        "all_words = ' '.join(df['ai_instr_clean']).split()\n",
        "word_freq = Counter(all_words)\n",
        "common_words = word_freq.most_common(10)\n",
        "\n",
        "# prepare for plotting\n",
        "words, counts = zip(*common_words)\n",
        "plt.figure(figsize=(12,5))\n",
        "plt.bar(words, counts, color='skyblue')\n",
        "plt.title('Top 10 Most Common Words in Cleaned AI Instruction Responses')\n",
        "plt.xlabel('Word')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "23fAaJggd8Si"
      },
      "outputs": [],
      "source": [
        "from wordcloud import WordCloud\n",
        "from collections import Counter\n",
        "\n",
        "text = ' '.join(df['ai_instr_clean'])\n",
        "\n",
        "# Check if there is any text left after cleaning and filtering\n",
        "if not text.strip():\n",
        "    print(\"No text remaining after cleaning and filtering for academic integrity responses. Skipping word cloud.\")\n",
        "else:\n",
        "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n",
        "    # create wordcloud\n",
        "    plt.figure(figsize=(12,6))\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.axis('off')\n",
        "    plt.title('Word Cloud of Cleaned AI Instruction Responses')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hzp5FOse5fFp"
      },
      "outputs": [],
      "source": [
        "print(df.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36Tu-7VREIEn"
      },
      "source": [
        "**OPEN ENDED DESCRIBED BENEFITS AND CHALLENGES**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TMxPr1vWEMtf"
      },
      "outputs": [],
      "source": [
        "# benefits / challenges unique responses\n",
        "df['If you have experienced benefits or challenges, please describe them.'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h33sRxqHEVOw"
      },
      "outputs": [],
      "source": [
        "# prepare stopwords\n",
        "stop_words_benefits = set(stopwords.words('english'))\n",
        "custom_stopwords_benefits = {\n",
        "    # Generic starters\n",
        "    \"i\", \"i'm\", \"i’ve\", \"i've\", \"i am\", \"i find\", \"i use\", \"i think\", \"i feel\", \"i personally\",\n",
        "    \"me\", \"my\", \"mine\", \"we\", \"our\", \"us\", \"don't\",\n",
        "\n",
        "    # Common filler and vague terms\n",
        "    \"helpful\", \"helped\", \"help\", \"useful\", \"use\", \"used\", \"using\", \"benefit\", \"benefits\",\n",
        "    \"challenge\", \"challenges\", \"issue\", \"issues\", \"problem\", \"problems\", \"sometimes\", \"however\",\n",
        "    \"sometimes\", \"often\", \"usually\", \"always\", \"also\", \"though\", \"but\", \"still\", \"yet\", \"very\",\n",
        "    \"lot\", \"many\", \"much\", \"more\", \"most\", \"some\", \"few\", \"just\", \"even\", \"well\",\n",
        "\n",
        "    # AI-specific generalities\n",
        "    \"ai\", \"chatgpt\", \"chat\", \"gpt\", \"gemini\", \"dall-e\", \"midjourney\", \"tools\", \"tool\", \"system\",\n",
        "    \"systems\", \"platform\", \"platforms\", \"google\", \"quizlet\", \"grammarly\", \"canva\", \"AI\", \"AI's\",\n",
        "    \"ais\",\n",
        "\n",
        "    # Common educational terms (broad & repetitive)\n",
        "    \"student\", \"students\", \"professor\", \"professors\", \"teacher\", \"teachers\", \"assignment\",\n",
        "    \"assignments\", \"class\", \"classes\", \"course\", \"courses\", \"homework\", \"school\", \"university\",\n",
        "    \"education\", \"study\", \"studying\", \"learning\", \"research\", \"paper\", \"papers\", \"essay\", \"essays\",\n",
        "    \"test\", \"tests\", \"quiz\", \"quizzes\", \"problem\", \"problems\", \"concept\", \"concepts\", \"content\",\n",
        "    \"benefit\", \"challenge\", \"benefits\", \"challenges\",\n",
        "\n",
        "    # Generic tech/academic terms\n",
        "    \"code\", \"coding\", \"script\", \"scripts\", \"module\", \"modules\", \"terraform\", \"aws\",\n",
        "    \"debug\", \"debugging\", \"solution\", \"solutions\", \"topic\", \"topics\", \"information\",\n",
        "    \"data\", \"explain\", \"explanation\", \"explained\", \"break down\", \"breakdown\",\n",
        "    \"question\", \"questions\", \"answer\", \"answers\",\n",
        "\n",
        "    # Non-informative sentiment or opinion expressions\n",
        "    \"good\", \"bad\", \"right\", \"wrong\", \"correct\", \"incorrect\", \"efficient\", \"inefficient\",\n",
        "    \"beneficial\", \"amazing\", \"great\", \"awesome\", \"fast\", \"slow\", \"better\", \"worse\",\n",
        "    \"positive\", \"negative\", \"useful\", \"useless\", \"accurate\", \"inaccurate\",\n",
        "\n",
        "    # Transitional and conversational phrases\n",
        "    \"that being said\", \"on that note\", \"for example\", \"e.g.\", \"such as\", \"versus\", \"vs\",\n",
        "    \"etc\", \"and so on\", \"as well as\", \"in order to\", \"at the end of the day\", \"to be honest\",\n",
        "    \"honestly\", \"basically\", \"essentially\", \"kind of\", \"sort of\",\n",
        "\n",
        "    # Environmental/political filler\n",
        "    \"environment\", \"expensive\", \"water\", \"waste\", \"cost\", \"capitalism\", \"greed\", \"agenda\",\n",
        "\n",
        "    # Empty responses\n",
        "    \"\", \" \", \".\", \",\", \"-\", \"—\", \"\\n\"\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "knYYc2oJFBwv"
      },
      "outputs": [],
      "source": [
        "# define cleaning function\n",
        "stop_words_benefits = set(stopwords.words('english')).union(custom_stopwords_benefits)\n",
        "def ai_benefit_preprocess(text):\n",
        "    if not isinstance(text, str):\n",
        "        return ''\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "    tokens = text.split()\n",
        "    tokens = [word for word in tokens if word not in stop_words_benefits]\n",
        "    return ' '.join(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OEoQtBarFG3P"
      },
      "outputs": [],
      "source": [
        "# apply cleaning\n",
        "df['ai_benefit_clean'] = df['If you have experienced benefits or challenges, please describe them.'].apply(ai_benefit_preprocess)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "67T0Bpy_FS_K"
      },
      "outputs": [],
      "source": [
        "# remove extra spaces\n",
        "df['ai_benefit_clean'] = df['ai_benefit_clean'].str.replace(r'\\s+', ' ', regex=True).str.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TTKOuFwVFae3"
      },
      "outputs": [],
      "source": [
        "# spot check\n",
        "print(df[['If you have experienced benefits or challenges, please describe them.', 'ai_benefit_clean']].head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tq7zXW3AFnhk"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# flatten all words into a single list\n",
        "all_words = ' '.join(df['ai_benefit_clean']).split()\n",
        "word_freq = Counter(all_words)\n",
        "common_words = word_freq.most_common(10)\n",
        "\n",
        "# prepare for plotting\n",
        "words, counts = zip(*common_words)\n",
        "plt.figure(figsize=(12,5))\n",
        "plt.bar(words, counts, color='skyblue')\n",
        "plt.title('Top 10 Most Common Words in Cleaned AI Benefit / Concern Responses')\n",
        "plt.xlabel('Word')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GUQrIsiWFyKx"
      },
      "outputs": [],
      "source": [
        "from wordcloud import WordCloud\n",
        "from collections import Counter # Import Counter\n",
        "\n",
        "text = ' '.join(df['ai_benefit_clean'])\n",
        "\n",
        "if not text.strip():\n",
        "    print(\"No text remaining after cleaning and filtering for academic integrity responses. Skipping word cloud.\")\n",
        "else:\n",
        "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n",
        "    # create wordcloud\n",
        "    plt.figure(figsize=(12,6))\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.axis('off')\n",
        "    plt.title('Word Cloud of Cleaned AI Benefit / Concern Responses')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0ViP0q7OULx"
      },
      "source": [
        "**NOTICEABLE IMPACT ON ACADEMIC INTEGRITY**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yh8sgikFQCX-"
      },
      "outputs": [],
      "source": [
        "# unique values for academic integrity open ended question\n",
        "df['Have you noticed any impact on your academic integrity or ethical considerations when using AI tools? Please explain.'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QMq5hBvUNifm"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "stop_words_integrity = set(stopwords.words('english'))\n",
        "custom_stopwords_integrity = {\n",
        "    # Standard filler and structure words\n",
        "    \"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\",\n",
        "    \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\",\n",
        "    \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\",\n",
        "    \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\",\n",
        "    \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\",\n",
        "    \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\",\n",
        "    \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\",\n",
        "    \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\",\n",
        "    \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\",\n",
        "    \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\",\n",
        "    \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\",\n",
        "    \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\",\n",
        "    \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\",\n",
        "    \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\",\n",
        "    \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\",\n",
        "    \"can\", \"will\", \"just\", \"don\", \"should\", \"now\", \"dont\", \"like\",\n",
        "\n",
        "    # Context-specific to AI/academic reflection filler\n",
        "    \"use\", \"using\", \"used\", \"uses\", \"ai\", \"chatgpt\", \"gpt\", \"gemini\", \"tools\",\n",
        "    \"tool\", \"assignment\", \"assignments\", \"class\", \"classes\", \"course\", \"courses\",\n",
        "    \"work\", \"school\", \"homework\", \"project\", \"projects\", \"paper\", \"papers\",\n",
        "    \"professor\", \"professors\", \"teacher\", \"teachers\", \"student\", \"students\",\n",
        "    \"college\", \"uri\", \"university\", \"major\", \"learning\", \"learn\", \"understand\",\n",
        "    \"understanding\", \"material\", \"topic\", \"topics\", \"concept\", \"concepts\",\n",
        "    \"resource\", \"resources\", \"thing\", \"things\", \"yes\",\n",
        "\n",
        "    # Meta/reflection words\n",
        "    \"think\", \"feel\", \"felt\", \"believe\", \"know\", \"noticed\",\n",
        "    \"sometimes\", \"often\", \"rarely\", \"usually\", \"always\", \"never\",\n",
        "    \"lot\", \"bit\", \"much\", \"many\", \"somewhat\", \"still\", \"personally\",\n",
        "    \"honestly\", \"however\", \"though\", \"really\", \"sure\", \"yes\",\n",
        "\n",
        "    # Education-related filler\n",
        "    \"quiz\", \"quizzes\", \"exam\", \"exams\", \"grade\", \"grades\", \"syllabus\", \"gpa\",\n",
        "    \"score\", \"points\", \"submission\", \"submit\", \"submitting\", \"due\", \"turn\",\n",
        "    \"turned\", \"test\", \"tests\", \"teacher\", \"professor\", \"assignment\", \"assignments\",\n",
        "\n",
        "    # Time references\n",
        "    \"now\", \"then\", \"later\", \"before\", \"after\", \"recently\", \"today\",\n",
        "    \"semester\", \"week\", \"weeks\", \"month\", \"months\", \"year\", \"years\",\n",
        "\n",
        "    # Neutral verbs/adverbs not useful for trends\n",
        "    \"get\", \"got\", \"make\", \"made\", \"doing\", \"done\", \"go\", \"went\", \"come\",\n",
        "    \"came\", \"see\", \"seen\", \"look\", \"looking\", \"take\", \"taking\", \"find\",\n",
        "    \"found\", \"give\", \"gave\", \"help\", \"helped\", \"helps\", \"helping\",\n",
        "    \"try\", \"tried\", \"trying\", \"start\", \"started\", \"starting\", \"would\",\n",
        "\n",
        "    # Modifiers that don’t add topical value\n",
        "    \"good\", \"bad\", \"better\", \"best\", \"worse\", \"important\", \"big\",\n",
        "    \"small\", \"easy\", \"easier\", \"hard\", \"harder\", \"difficult\", \"simple\",\n",
        "    \"fast\", \"slow\", \"quick\", \"quickly\", \"time\", \"times\", \"lot\", \"loads\",\n",
        "\n",
        "    # Punctuation & artifacts\n",
        "    \"\", \".\", \",\", \"“\", \"”\", \"\\\"\", \"\\\"\", \"’\", \"–\", \"-\", \"—\", \"\\n\",\n",
        "\n",
        "    # Redundant common sense pairs\n",
        "    \"ethical\", \"ethics\", \"integrity\", \"impact\", \"consideration\", \"considerations\"\n",
        "}\n",
        "stop_words_integrity.update(custom_stopwords_integrity)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ef482_POatM"
      },
      "outputs": [],
      "source": [
        "# cleaning function\n",
        "def integrity_preprocess(text):\n",
        "    if not isinstance(text, str):\n",
        "        return ''\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "    tokens = text.split()\n",
        "    tokens = [word for word in tokens if word not in stop_words_integrity]\n",
        "    return ' '.join(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OPuTp4tnOuKc"
      },
      "outputs": [],
      "source": [
        "# apply cleaning to data\n",
        "df['ai_integrity_clean'] = df['Have you noticed any impact on your academic integrity or ethical considerations when using AI tools? Please explain.'].apply(integrity_preprocess)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S9unRRJsO_A0"
      },
      "outputs": [],
      "source": [
        "# remove extra spaces\n",
        "df['ai_integrity_clean'] = df['ai_integrity_clean'].str.replace(r'\\s+', ' ', regex=True).str.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VnfqKRnuPSNg"
      },
      "outputs": [],
      "source": [
        "# spot check\n",
        "for i in range(5):\n",
        "    print(\"Original:\", df['Have you noticed any impact on your academic integrity or ethical considerations when using AI tools? Please explain.'].iloc[i])\n",
        "    print(\"Cleaned: \", df['ai_integrity_clean'].iloc[i])\n",
        "    print(\"------\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d0tGLV-mnf49"
      },
      "outputs": [],
      "source": [
        "# flatten all words\n",
        "all_words_integrity = ' '.join(df['ai_integrity_clean']).split()\n",
        "word_freq_integrity = Counter(all_words_integrity)\n",
        "common_words_integrity = word_freq_integrity.most_common(10)\n",
        "\n",
        "# prepare for plotting\n",
        "words_integrity, counts_integrity = zip(*common_words_integrity)\n",
        "plt.figure(figsize=(12,5))\n",
        "plt.bar(words_integrity, counts_integrity, color='skyblue')\n",
        "plt.title('Top 10 Most Common Words in Cleaned AI Integrity Responses')\n",
        "plt.xlabel('Word')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L9Cwgiaaob0J"
      },
      "outputs": [],
      "source": [
        "# create wordcloud\n",
        "text_integrity = ' '.join(df['ai_integrity_clean'])\n",
        "\n",
        "plt.figure(figsize=(12,6))\n",
        "wordcloud_integrity = WordCloud(width=800, height=400, background_color='white').generate(text_integrity)\n",
        "plt.imshow(wordcloud_integrity, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.title('Word Cloud of Cleaned AI Integrity Responses')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "meJcxCnr5dab"
      },
      "outputs": [],
      "source": [
        "print(df.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLyvkqknpLq2"
      },
      "source": [
        "**CLEANING DATA REGARDING AI TOOLS SUGGESTIONS OR CONCERNS**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8C33rWGypXRU"
      },
      "outputs": [],
      "source": [
        "# unique values\n",
        "df['What suggestions or concerns do you have about AI tools being integrated into education?'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1AMJtt4tp70f"
      },
      "outputs": [],
      "source": [
        "# prepare suggestions / concerns stopwords\n",
        "stop_words_suggestions = set(stopwords.words('english'))\n",
        "custom_stopwords_suggestions = {\n",
        "    # Common vague/general verbs & phrases\n",
        "    \"think\", \"feel\", \"believe\", \"know\", \"wish\", \"want\", \"say\", \"saying\", \"said\",\n",
        "    \"concern\", \"concerns\", \"suggest\", \"suggestions\", \"idea\", \"ideas\",\n",
        "    \"recommend\", \"recommendation\", \"recommendations\", \"use\", \"used\", \"using\",\n",
        "    \"uses\", \"going\", \"people\", \"make\", \"makes\", \"made\", \"doing\", \"do\", \"done\",\n",
        "    \"did\", \"get\", \"got\", \"give\", \"gave\", \"have\", \"has\", \"had\", \"need\",\n",
        "    \"needs\", \"needed\", \"should\", \"would\", \"could\", \"can\", \"may\", \"might\",\n",
        "    \"will\", \"must\",\n",
        "\n",
        "    # Filler or reflection phrasing\n",
        "    \"personally\", \"honestly\", \"just\", \"actually\", \"really\", \"very\", \"so\",\n",
        "    \"maybe\", \"probably\", \"definitely\", \"totally\", \"somehow\", \"somewhat\",\n",
        "    \"kind of\", \"sort of\", \"still\", \"often\", \"sometimes\", \"always\", \"never\",\n",
        "    \"usually\", \"ever\", \"already\", \"also\",\n",
        "\n",
        "    # AI reference overuse\n",
        "    \"ai\", \"artificial\", \"intelligence\", \"chatgpt\", \"gpt\", \"generative\", \"tool\",\n",
        "    \"tools\", \"llm\", \"technology\", \"technologies\", \"system\", \"systems\",\n",
        "    \"platform\", \"platforms\",\n",
        "\n",
        "    # Education context redundancy\n",
        "    \"student\", \"students\", \"teacher\", \"teachers\", \"professor\", \"professors\",\n",
        "    \"assignment\", \"assignments\", \"exam\", \"exams\", \"test\", \"tests\", \"homework\",\n",
        "    \"class\", \"classes\", \"course\", \"courses\", \"school\", \"education\", \"learning\",\n",
        "    \"studying\", \"grade\", \"grades\", \"paper\", \"papers\", \"project\", \"projects\",\n",
        "\n",
        "    # Generic academic/future references\n",
        "    \"future\", \"career\", \"job\", \"jobs\", \"life\", \"lives\", \"world\", \"society\",\n",
        "    \"generation\", \"college\", \"university\", \"uri\", \"pre-med\", \"pre-vet\",\n",
        "\n",
        "    # Overused general nouns/adjectives\n",
        "    \"thing\", \"things\", \"stuff\", \"person\", \"everyone\", \"anyone\",\n",
        "    \"someone\", \"nothing\", \"everything\", \"something\", \"issue\", \"issues\",\n",
        "    \"problem\", \"problems\", \"point\", \"way\", \"extent\", \"time\", \"times\", \"place\",\n",
        "    \"situation\", \"good\", \"bad\", \"better\", \"best\", \"scary\", \"danger\",\n",
        "    \"dangerous\", \"important\", \"big\", \"small\", \"helpful\", \"useful\", \"harmful\",\n",
        "    \"wrong\", \"right\", \"ethical\", \"unethical\", \"moral\", \"morals\",\n",
        "\n",
        "    # Expressions or filler statements (multi-word!)\n",
        "    \"i think\", \"i believe\", \"i feel\", \"i know\", \"in my opinion\", \"i would say\",\n",
        "    \"at least\", \"as long as\", \"as a future\", \"from my experience\",\n",
        "    \"in the future\", \"that being said\", \"with that said\", \"come on\", \"hold on\",\n",
        "    \"to some extent\", \"to an extent\",\n",
        "\n",
        "    # Redundant meta language\n",
        "    \"concerned\", \"scared\", \"worried\", \"afraid\", \"fear\", \"fearful\",\n",
        "    \"cheating\", \"cheat\", \"cheats\", \"cheated\", \"plagiarism\", \"false\", \"falsely\",\n",
        "\n",
        "    # Policy and general system references\n",
        "    \"rule\", \"rules\", \"policy\", \"policies\", \"regulation\", \"regulations\", \"law\",\n",
        "    \"laws\",\n",
        "\n",
        "    # Misuse repetition\n",
        "    \"rely\", \"relying\", \"depend\", \"depending\", \"abuse\", \"abusing\", \"overuse\",\n",
        "    \"overusing\",\n",
        "\n",
        "    # Empty & punctuation\n",
        "    \"\", \" \", \".\", \",\", \"’\", \"“\", \"”\", \"'\", \"\\\"\", \"-\", \"—\", \"\\n\"\n",
        "}\n",
        "stop_words_suggestions.update(custom_stopwords_suggestions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rYrNtBWLqVH1"
      },
      "outputs": [],
      "source": [
        "# define cleaning function\n",
        "def ai_suggestions_preprocess(text):\n",
        "    if not isinstance(text, str):\n",
        "        return ''\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "    tokens = text.split()\n",
        "    tokens = [word for word in tokens if word not in stop_words_suggestions]\n",
        "    return ' '.join(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ibjCYOgtqhh7"
      },
      "outputs": [],
      "source": [
        "# apply cleaning\n",
        "df['ai_suggestions_clean'] = df[\"What suggestions or concerns do you have about AI tools being integrated into education?\"].apply(ai_suggestions_preprocess)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A4GijZXbq2U0"
      },
      "outputs": [],
      "source": [
        "# remove extra spaces\n",
        "df['ai_suggestions_clean'] = df['ai_suggestions_clean'].str.replace(r'\\s+', ' ', regex=True).str.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fM8zqdSLrW_F"
      },
      "outputs": [],
      "source": [
        "# spot check\n",
        "for i in range(5):\n",
        "    print(\"Original:\", df[\"What suggestions or concerns do you have about AI tools being integrated into education?\"].iloc[i])\n",
        "    print(\"Cleaned: \", df['ai_integrity_clean'].iloc[i])\n",
        "    print(\"------\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8gyfErbZrcVj"
      },
      "outputs": [],
      "source": [
        "# flatten all words into single list\n",
        "all_words_suggestions = ' '.join(df['ai_suggestions_clean']).split()\n",
        "word_freq_suggestions = Counter(all_words_suggestions)\n",
        "common_words_suggestions = word_freq_suggestions.most_common(10)\n",
        "\n",
        "# prepare for plotting\n",
        "words, counts = zip(*common_words_suggestions)\n",
        "plt.figure(figsize=(12,5))\n",
        "plt.bar(words, counts, color='skyblue')\n",
        "plt.title('Top 10 Most Common Words in Cleaned AI Suggestions / Concerns Responses')\n",
        "plt.xlabel('Word')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h_Ffr2fdtOIf"
      },
      "outputs": [],
      "source": [
        "# create wordcloud\n",
        "text_suggestions = ' '.join(df['ai_suggestions_clean'])\n",
        "\n",
        "plt.figure(figsize=(12,6))\n",
        "wordcloud_suggestions = WordCloud(width=800, height=400, background_color='white').generate(text_suggestions)\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.title('Word Cloud of Cleaned AI Suggestions / Concerns Responses')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n0Ziqwubs6v9"
      },
      "outputs": [],
      "source": [
        "print(df.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PtKpp5KZvB5D"
      },
      "source": [
        "**CLEANING OPEN STUDENT OPINIONS AND EXPERIENCE RESPONSES**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bmNGn9miw46Z"
      },
      "outputs": [],
      "source": [
        "df['Is there anything else you would like to share about your opinions and experiences regarding AI tools and student learning?'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9UIG224txt9k"
      },
      "outputs": [],
      "source": [
        "# stop words for open ended opinions\n",
        "stop_words_opinions = set(stopwords.words('english'))\n",
        "custom_stopwords_opinions = {\n",
        "    # Filler or vague starters\n",
        "    \"i think\", \"i believe\", \"i feel\", \"in my opinion\", \"just\", \"actually\", \"honestly\",\n",
        "    \"personally\", \"it's okay\", \"it's not like\", \"i don’t like\", \"i like\",\n",
        "    \"not everyone\", \"not all\", \"i'm sorry\", \"thank you\", \"thanks\", \"no\", \"none\",\n",
        "    \"none in particular\", \"i hope\", \"i have too much to say\",\n",
        "    \"i don't really have much else to say\", \"use\", \"used\",\n",
        "\n",
        "    # Overused AI references\n",
        "    \"ai\", \"artificial intelligence\", \"generative ai\", \"chatgpt\", \"tools\",\n",
        "    \"tool\", \"these tools\", \"ai tools\", \"machine\", \"systems\", \"platform\", \"platforms\",\n",
        "\n",
        "    # Common educational context\n",
        "    \"students\", \"student\", \"teacher\", \"teachers\", \"university\", \"college\", \"school\",\n",
        "    \"education\", \"learning\", \"courses\", \"course\", \"assignments\", \"assignment\",\n",
        "    \"class\", \"classes\", \"credits\", \"material\", \"professor\", \"professors\",\n",
        "\n",
        "    # Redundant academic references\n",
        "    \"cs\", \"computer science\", \"math\", \"science\", \"physics\",\n",
        "\n",
        "    # Broad social commentary\n",
        "    \"world\", \"society\", \"the future\", \"future\", \"generation\", \"we're cooked\",\n",
        "    \"we are cooked\", \"get left behind\", \"bad reputation\", \"corporate greed\",\n",
        "    \"capitalism\", \"mass surveillance\", \"agenda\", \"palantir\",\n",
        "\n",
        "    # Generic statements or overused sentiments\n",
        "    \"ai is helpful\", \"ai is powerful\", \"ai is the future\", \"ai has changed the world\",\n",
        "    \"super human capabilities\", \"easier and easier\", \"it can be used\",\n",
        "    \"it should be\", \"it shouldn't be\", \"should be avoided\", \"it’s difficult to regulate\",\n",
        "    \"has helped me\", \"helped me learn\", \"learn how to use\", \"it can and will\",\n",
        "    \"humans will lead themselves into destruction\", \"this is part of their agenda\",\n",
        "    \"i always say please and thank you\", \"its only a matter of time\",\n",
        "\n",
        "    # Misused or shallow ethical claims\n",
        "    \"integrity\", \"cheating\", \"cheat\", \"cheated\", \"abuse\", \"abused\", \"using it to cheat\",\n",
        "\n",
        "    # Misc/empty punctuation\n",
        "    \"\", \" \", \".\", \",\", \"’\", \"“\", \"”\", \"'\", \"\\\"\", \"-\", \"—\", \"\\n\"\n",
        "}\n",
        "stop_words_opinions.update(custom_stopwords_opinions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pqa7C3VMyXQu"
      },
      "outputs": [],
      "source": [
        "# define cleaning function\n",
        "stop_words_opinions = set(stopwords.words('english'))\n",
        "stop_words_opinions.update(custom_stopwords_opinions)\n",
        "\n",
        "def ai_opinions_preprocess(text):\n",
        "    if not isinstance(text, str):\n",
        "        return ''\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "    tokens = text.split()\n",
        "    tokens = [word for word in tokens if word not in stop_words_opinions]  # <-- FIXED HERE\n",
        "    return ' '.join(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R9ZQELw-ygSA"
      },
      "outputs": [],
      "source": [
        "# apply cleaning\n",
        "df['ai_opinions_clean'] = df['Is there anything else you would like to share about your opinions and experiences regarding AI tools and student learning?'].apply(ai_opinions_preprocess)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jv5HbUShyn2B"
      },
      "outputs": [],
      "source": [
        "# remove extra spaces\n",
        "df['ai_opinions_clean'] = df['ai_opinions_clean'].str.replace(r'\\s+', ' ', regex=True).str.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ap6CjNFKzUOV"
      },
      "outputs": [],
      "source": [
        "# spot check\n",
        "for i in range(5):\n",
        "    print(\"Original:\", df['Is there anything else you would like to share about your opinions and experiences regarding AI tools and student learning?'].iloc[i])\n",
        "    print(\"Cleaned: \", df['ai_opinions_clean'].iloc[i])\n",
        "    print(\"------\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hQRngpBCz_fU"
      },
      "outputs": [],
      "source": [
        "# flatten all words\n",
        "all_words_opinions = ' '.join(df['ai_opinions_clean']).split()\n",
        "word_freq_opinions = Counter(all_words_opinions)\n",
        "common_words_opinions = word_freq_opinions.most_common(10)\n",
        "\n",
        "# prepare for plotting\n",
        "words, counts = zip(*common_words_opinions)\n",
        "plt.figure(figsize=(12,5))\n",
        "plt.bar(words, counts, color='skyblue')\n",
        "plt.title('Top 10 Most Common Words in Cleaned AI Opinions Responses')\n",
        "plt.xlabel('Word')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "alErYSmF5CBh"
      },
      "outputs": [],
      "source": [
        "# creating wordcloud\n",
        "text_opinions = ' '.join(df['ai_opinions_clean'])\n",
        "\n",
        "plt.figure(figsize=(12,6))\n",
        "wordcloud_opinions = WordCloud(width=800, height=400, background_color='white').generate(text_opinions)\n",
        "plt.imshow(wordcloud_opinions, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.title('Word Cloud of Cleaned AI Opinions Responses')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bvphayzDswlp"
      },
      "outputs": [],
      "source": [
        "print(df.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2k8kw5XJwjG6"
      },
      "source": [
        "The method used to encode open ended responses is observably consistent between each column. With that being said, this marks the end of the data preprocessing, with all 17 rows effectively encoded and cleaned. Now, this can be used to suggest findings in the survey data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E69V-bjQ5QzZ"
      },
      "source": [
        "# Pattern Emergence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BT2Pe63L5Wgq"
      },
      "source": [
        "At this point, every single column is accounted for in cleaning. Since everything is interpretable my python, now everything can potentially be applied to finding distinct patterns in data. This section will be primiarly dedicated to visualizations and basic, effective machine learning practices."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxUxYjh58fsN"
      },
      "source": [
        "**GENERAL PATTERNS FROM RESPONSES**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQtI_VIO_YXs"
      },
      "source": [
        "1. AI tool use is widely recognizable to students both in concept and practice.\n",
        "\n",
        "*   Nearly every respondent are aware of AI tools and almost all of them have tried at least one tool\n",
        "*   Even students that are less that familiar with these tools have some exposure\n",
        "*   The frequency in which students use these tools varies widely; daily engagement to occasional, some use AI tools for specific academic tasks and some have integrated it into generic study routines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0xHeb9MkPRVm"
      },
      "outputs": [],
      "source": [
        "print(df['familiarity_category'].unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ui4u_GtHPV4M"
      },
      "outputs": [],
      "source": [
        "category_labels = ['Slightly Familiar', 'Somewhat Familiar', 'Quite Familiar', 'Very Familiar']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X7JyC8fZPWwu"
      },
      "outputs": [],
      "source": [
        "counts = df['familiarity_category'].value_counts().reindex(category_labels, fill_value=0)\n",
        "print(counts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_pbvnif8PZUg"
      },
      "outputs": [],
      "source": [
        "colors = ['#ff9999','#66b3ff','#99ff99','#ffcc99']  # Or your preferred palette\n",
        "\n",
        "plt.figure(figsize=(7,7))\n",
        "patches, texts, autotexts = plt.pie(\n",
        "    counts,\n",
        "    labels=category_labels,\n",
        "    autopct='%1.1f%%',\n",
        "    colors=colors,\n",
        "    startangle=140,\n",
        "    textprops={'fontsize': 12},\n",
        "    wedgeprops={'edgecolor': 'white'}\n",
        ")\n",
        "plt.setp(autotexts, size=13, weight='bold')\n",
        "plt.title('Respondent Familiarity with AI Tools', fontsize=16, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1J0NvtWcXi4"
      },
      "source": [
        "2. Typical Purposes of AI tool use ranges\n",
        "\n",
        "*   Some of the most adopted academic utilities from AI include the following:\n",
        "    1. Studying (summarizing notes, creating study guides, explaining practice problems)\n",
        "    2. Writing (proofreading, brainstorming, essay structure)\n",
        "    3. Research (brainstorming project ideas, finding information)  \n",
        "\n",
        "*   Some students specifically use AI only for mundane or repetitive tasks\n",
        "*   Some students rely on AI tools to explain complex concepts\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hLHt5-EXdLMU"
      },
      "outputs": [],
      "source": [
        "# pie chart for tool tasks\n",
        "plt.figure(figsize=(6, 6))\n",
        "colors = plt.get_cmap('tab20').colors[:len(filtered_task_counts)]\n",
        "patches, texts, autotexts = plt.pie(\n",
        "    filtered_task_counts.values,\n",
        "    labels=filtered_task_counts.index,\n",
        "    autopct='%1.1f%%',\n",
        "    startangle=140,\n",
        "    colors=colors,\n",
        "    textprops={'fontsize': 12},\n",
        "    wedgeprops={'edgecolor': 'white'}\n",
        ")\n",
        "plt.setp(autotexts, size=13, weight='bold')\n",
        "plt.title('AI Tasks Usage - Pie Chart', fontsize=16, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MpDODMZ0didv"
      },
      "source": [
        "3. Variety of perceived benefits\n",
        "\n",
        "\n",
        "*   One of the most common responses towards this topic include that AI can perform tedious tasks much faster than a student can; this allows students to focus on more time consuming and difficult components of their classwork\n",
        "*   Students praise Generative AI's ability to explain a vast amount of concepts in varying degrees of detail; this has garnered the student notion that it can be a \"24/7 tutor\"\n",
        "*   Some students appreciate the potential of AI to help with skills that are not typically reinforced in college level classes (i.e. grammar with Grammarly)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMKPULyshG58"
      },
      "source": [
        "4. Variety of listed challenges and concerns\n",
        "\n",
        "\n",
        "*   One prominent student concern is that AI can provide information is either incorrect or misleading; the other hand of Generative AI explaining sophisticated topics is that these types of questions are most likely to be misinformed\n",
        "*   Some students worry that the embrace of AI in student learning has directly contributed to the erosion of ethical norms and considerations in student learning\n",
        "    1.   Students may become overly reliant on these platforms which has a diminishing impact on critical thinking and problem solving skills\n",
        "    2.   Students may believe that others are inherently using AI to frauduently complete classwork and assignments\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kUDaRszrp2mI"
      },
      "outputs": [],
      "source": [
        "# create wordcloud\n",
        "text_suggestions = ' '.join(df['ai_suggestions_clean'])\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "wordcloud_suggestions = WordCloud(width=800, height=400, background_color='white').generate(text_suggestions)\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.title('Word Cloud of Cleaned AI Suggestions / Concerns Responses')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VpGMZ0CVYmA"
      },
      "source": [
        "5. Students are confused by Professor's instruction of AI policies\n",
        "\n",
        "\n",
        "*   Student experience regarding professional enforcement of AI tools is largely inconsistent\n",
        "    *   Some professors willingly encourage AI use for specific assignments, while others may provide strict prohibitions for using these tools\n",
        "    *   Some professors allow AI for brainstorming and research but are not allowed for assignment completion\n",
        "*   Common student sentiment indicates that clearer and more consistent instruction on how to use AI tools in educational settings would lessen their worry about ethical concerns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EGaiQyujp43X"
      },
      "outputs": [],
      "source": [
        "from wordcloud import WordCloud\n",
        "from collections import Counter\n",
        "\n",
        "text = ' '.join(df['ai_instr_clean'])\n",
        "\n",
        "# Check if there is any text left after cleaning and filtering\n",
        "if not text.strip():\n",
        "    print(\"No text remaining after cleaning and filtering for academic integrity responses. Skipping word cloud.\")\n",
        "else:\n",
        "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n",
        "    # create wordcloud\n",
        "    plt.figure(figsize=(10,6))\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.axis('off')\n",
        "    plt.title('Word Cloud of Cleaned AI Instruction Responses')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "by4hXUZzWW3q"
      },
      "source": [
        "6. STEM majors use AI more frequently than non-STEM majors\n",
        "\n",
        "*   Not only do STEM students use AI tools more, but also use a greater variety of tools\n",
        "*   Non-STEM students primarily use these tools for writing or brainstorming purposes; STEM students most often use these tools for technical tasks (i.e. explaining complex concepts, programming)\n",
        "*   Upperclassmen and graduate students approach AI tools in more nuanced AI tool use or provide more critical perspectives; Freshman and Sophomores describe more limited or experimental approachs with their use"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_9CNu75eqw6F"
      },
      "outputs": [],
      "source": [
        "# soft pastel\n",
        "colors = ['#f7cac9', '#92b4ec', '#fff2b2', '#b6e2d3', '#f6eac2', '#c9c9ff', '#ffb3e6', '#b3ffb3'] # Added colors for all major categories\n",
        "\n",
        "# personal modifications\n",
        "grade_counts_major = df['major_category'].value_counts().sort_index()\n",
        "labels_major = grade_counts_major.index\n",
        "sizes_major = grade_counts_major.values\n",
        "\n",
        "# pie chart of major distribution\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.pie(\n",
        "    sizes_major,\n",
        "    labels=labels_major,\n",
        "    autopct='%1.1f%%',\n",
        "    startangle=90,\n",
        "    colors=colors,\n",
        "    wedgeprops={'edgecolor': 'white', 'linewidth': 2}\n",
        ")\n",
        "plt.title(f'Major Distribution', fontsize=15, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1iyuBm2JvT6o"
      },
      "source": [
        "Many of these repeated, basic visualizations provide relevant context about the information at hand. The next step to unlock further important insights is to use encoded data and compare the relationships between 2 or 3 different features. This would provide more reasoning behind the general findings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zu9tPBI3tj4c"
      },
      "source": [
        "# More Comprehensive Deep Dives"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DuEVdUKvtqtA"
      },
      "source": [
        "**GENDER USE OF AI TOOLS**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4JI3bZH9Lv-"
      },
      "source": [
        "Demographic data collected in the survey will serve as an important basis of comparison when determining discrpencies in AI tool use. First, there will be a displayed visualization to find the difference in how much AI tools are being used by each gender."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8v7Q1hDZ4p75"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YcUluW2pvqqR"
      },
      "outputs": [],
      "source": [
        "# mapping for plot labels\n",
        "gender_map = {1: 'Male', 2: 'Female', 3: 'Non-Binary'}\n",
        "frequency_map = {\n",
        "    0: 'Never or Rarely',\n",
        "    1: 'Monthly or Less',\n",
        "    2: 'Weekly',\n",
        "    3: 'Daily or More'\n",
        "}\n",
        "# more readable labels\n",
        "df['gender'] = df['gender_encoded'].map(gender_map)\n",
        "df['frequency'] = df['frequency_encoded'].map(frequency_map)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MGx4PVcj3-63"
      },
      "outputs": [],
      "source": [
        "# set both orders\n",
        "gender_order = ['Male', 'Female', 'Non-Binary']\n",
        "frequency_order = ['Never or Rarely', 'Monthly or Less', 'Weekly', 'Daily or More']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ia67xwvQIBBw"
      },
      "outputs": [],
      "source": [
        "# compute crosstab between features\n",
        "ct_genfreq = pd.crosstab(\n",
        "    df['gender'],\n",
        "    df['frequency']\n",
        ").reindex(index=gender_order, columns=frequency_order, fill_value=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AYhN9E6sIExe"
      },
      "outputs": [],
      "source": [
        "# normalize to compare features\n",
        "ct_genfreq_norm = ct_genfreq.div(ct_genfreq.sum(axis=1), axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7vqyK3_tINq6"
      },
      "outputs": [],
      "source": [
        "# plot comparitive bar graph\n",
        "ax = ct_genfreq_norm.plot(kind='bar', figsize=(10, 6), colormap='Set2', edgecolor='black')\n",
        "\n",
        "plt.title('Frequency of AI Tool Use by Gender', fontsize=16, fontweight='bold')\n",
        "plt.xlabel('Gender', fontsize=13, fontweight='bold')\n",
        "plt.ylabel('Proportion of Respondents', fontsize=13, fontweight='bold')\n",
        "plt.xticks(rotation=0)\n",
        "plt.legend(title='AI Tool Use Frequency', title_fontsize=13, fontsize=12)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nh2LnJUJ9h1B"
      },
      "source": [
        "Evidenced by the data presented in this bar graph, both men and women use AI tools but to somewhat varying degrees. The line chart demonstrates the mixed trends captured in the comparitive relationship.\n",
        "\n",
        "*   Men: far more likely to use AI tools in high abundance, answered each other response less prominently compared to women\n",
        "*   Women: share positive trends with men between the lowest three degress of use with a slightly higher response rate with all of those\n",
        "*   Non-Binary: not information to make any significant statements unfortunately\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YR8aq-LTkDUs"
      },
      "source": [
        "Why are men suggesting that use AI tools on a daily basis? Let's find out!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Imki9CskJRd"
      },
      "outputs": [],
      "source": [
        "# filter dataframe to only include male responses\n",
        "male_df = df[df['gender'] == 'Male']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DntecRg_9xOe"
      },
      "outputs": [],
      "source": [
        "# filter to daily ai tool use\n",
        "male_daily_df = male_df[male_df['frequency'] == 'Daily or More']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4VaSWdpF95ep"
      },
      "outputs": [],
      "source": [
        "# male daily wordcloud\n",
        "text_data = male_daily_df['If you have experienced benefits or challenges, please describe them.'].dropna().str.cat(sep=' ')\n",
        "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text_data)\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.title('Most Common Themes: Men Daily AI Tool Users')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CQOQUlW2FRCX"
      },
      "outputs": [],
      "source": [
        "# filter dataframe to only include female responses\n",
        "female_df = df[df['gender'] == 'Female']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cgO5XyeRJNpN"
      },
      "outputs": [],
      "source": [
        "# filter to daily ai tool use\n",
        "female_daily_df = female_df[female_df['frequency'] == 'Daily or More']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "niFcVyjLJQae"
      },
      "outputs": [],
      "source": [
        "# female daily wordcloud\n",
        "text_data = female_daily_df['If you have experienced benefits or challenges, please describe them.'].dropna().str.cat(sep=' ')\n",
        "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text_data)\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.title('Most Common Themes: Women Daily AI Tool Users')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HoCBXD_nJqhR"
      },
      "outputs": [],
      "source": [
        "!pip install textblob\n",
        "\n",
        "import pandas as pd\n",
        "from textblob import TextBlob\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sEksHLskKSHM"
      },
      "outputs": [],
      "source": [
        "# sentiment scoring function\n",
        "def get_sentiment(text):\n",
        "    # missing or non-string entries\n",
        "    if not isinstance(text, str) or text.strip() == \"\":\n",
        "        return None\n",
        "    return TextBlob(text).sentiment.polarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rNWxMGSiKWgw"
      },
      "outputs": [],
      "source": [
        "# apply sentiment analysis to both groups\n",
        "male_df['sentiment'] = male_df['If you have experienced benefits or challenges, please describe them.'].apply(get_sentiment)\n",
        "female_df['sentiment'] = female_df['If you have experienced benefits or challenges, please describe them.'].apply(get_sentiment)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qZCc4mvqKfEb"
      },
      "outputs": [],
      "source": [
        "# bar chart\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.hist(male_df['sentiment'].dropna(), bins=15, alpha=0.7, label='Male', color='blue')\n",
        "plt.hist(female_df['sentiment'].dropna(), bins=15, alpha=0.7, label='Female', color='pink')\n",
        "plt.xlabel('Sentiment Polarity', fontweight='bold')\n",
        "plt.ylabel('Number of Responses', fontweight='bold')\n",
        "plt.legend()\n",
        "plt.title('Sentiment Distribution: Benefits/Challenges Responses by Gender', fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBVRiiq4z3Cs"
      },
      "source": [
        "1. Benefits dominate both men and women, but still have noticable differences in response types.\n",
        "\n",
        "*   Men that effectively use AI tools everyday have suggested 3x the amount of responses that have suggested benefits, and are less likely to discuss potential drawbacks\n",
        "*   Women have indicated more mixed responses that likely acknowledge benefits, but are challenged by the potential downsides of utlizing the technology, or vice versa\n",
        "*   Very little of the open-ended responses held a completely negative connotation and the amount of people that never use the tools is also relatively small\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-DzrUxT1e3Y"
      },
      "source": [
        "2. Increased use of AI tools and perceived benefits do not have directly correlative trends.\n",
        "\n",
        "*   For both men and women, the highest amount of beneficial responses come from occasional use of AI tools\n",
        "*   The benefit / challenge column is more evenly spread among men while women have strong responses within the \"Other\" mark which leaves some of their responses up to interpretation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vU2DscRE6fuJ"
      },
      "source": [
        "Based on these provide outcomes from the visualizations, there are clear differences in how each gender experiences AI in education and their relection of its potential value and drawbacks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wqzScfl8bHE"
      },
      "source": [
        "**DIFFERENCE IN MAJOR USE OF AI TOOLS**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What are the most commonly used AI Tools within each major?"
      ],
      "metadata": {
        "id": "ZtGwlni9qpqS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aXRFA8HIDiux"
      },
      "outputs": [],
      "source": [
        "# explode ai tools list\n",
        "df_exploded = df.explode('ai_tools_list')\n",
        "# group by major category, tool, count\n",
        "major_tool_counts = df_exploded.groupby(['major_category', 'ai_tools_list']).size().reset_index(name='count')\n",
        "# for top 'n' tools by major\n",
        "top_n = 3\n",
        "top_major_tools = major_tool_counts.groupby('major_category').apply(lambda x: x.nlargest(top_n, 'count')).reset_index(drop=True)\n",
        "print(top_major_tools)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# bar chart\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x='major_category', y='count', hue='ai_tools_list', data=top_major_tools, palette='Set2')\n",
        "plt.title('Top 3 AI Tools Used by Major Category', fontsize=16, fontweight='bold')\n",
        "plt.xlabel('Major Category', fontsize=13, fontweight='bold')\n",
        "plt.ylabel('Count', fontsize=13, fontweight='bold')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.legend(title='AI Tools', title_fontsize=13, fontsize=12)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pdR6eorBrC9V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ChatGPT is a dominant force in AI tool use. This could easily apply outside of education as well, but its academic influence is substantial."
      ],
      "metadata": {
        "id": "9M2oaGtyrM_z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Are there disproportions in the popularity of AI tools for particular majors?"
      ],
      "metadata": {
        "id": "MUI5O1DtreqD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3mXZM9dWcu5e"
      },
      "outputs": [],
      "source": [
        "# normalize majors through total responses\n",
        "major_totals = df.groupby('major_category').size().rename('major_total').reset_index()\n",
        "# reuse exploded dataframe\n",
        "major_tool_counts = df_exploded.groupby(['major_category', 'ai_tools_list']).size().reset_index(name='count')\n",
        "\n",
        "# merge and calculate proportion\n",
        "major_prop_df = major_tool_counts.merge(major_totals, on='major_category')\n",
        "major_prop_df['proportion'] = major_prop_df['count'] / major_prop_df['major_total']\n",
        "print(major_prop_df.sort_values(['ai_tools_list', 'proportion'], ascending=[True, False]))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are a variety of results achieve from determining whether some AI tools are used overabudantly in some majors while being underutilized by others. Some have issues with sample size insensitivity where 4/8 students using a particular tool indicates it has the single largest plurality of any major. Canva seems to uniquely have a large effect in this section because it caters to majors with more specific needs for AI tools."
      ],
      "metadata": {
        "id": "BCzbe07zsYQb"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rnHyXYdQsZIn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_BDhzwf8sZLt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "K8f0beoisZbL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4htq2tzscI_Q"
      },
      "outputs": [],
      "source": [
        "# readable labels\n",
        "df['major_category'] = df['major_encoded'].map(code_to_category)\n",
        "df['Frequency'] = df['consistency_encoded'].map(frequency_labels)\n",
        "\n",
        "# summary table\n",
        "summary = pd.crosstab(df['Frequency'], df['major_category'])\n",
        "\n",
        "# sorting frequency categories\n",
        "ordered_frequencies = ['Rarely / Never', 'Occasionally', 'Regularly', 'Frequently / Daily']\n",
        "summary = summary.reindex(ordered_frequencies)\n",
        "\n",
        "# line chart\n",
        "plt.figure(figsize=(10, 6))\n",
        "for major in summary.columns:\n",
        "    plt.plot(summary.index, summary[major], marker='o', label=major)\n",
        "\n",
        "plt.title('AI Tool Usage Frequency by Major')\n",
        "plt.xlabel('Frequency of AI Tool Use')\n",
        "plt.ylabel('Number of Students')\n",
        "plt.legend(title='Major')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYAZFkIZo_JJ"
      },
      "source": [
        "There are a few basic, yet relevant insights to take from here:\n",
        "\n",
        "1.   STEM majors more frequently use AI in high abundance, but some of this may be skewed because more STEM students responed in general\n",
        "2.   Many arts, Education, and Social Science students use AI tools occasionally or regularly, but very few use them frequently / daily\n",
        "3.   That general difference in trends across all major classifications indicate that AI use frequency is strongly influenced by a students major or study\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z36V-UbJr4Nh"
      },
      "outputs": [],
      "source": [
        "# group by gender and frequency\n",
        "summary_major = pd.crosstab(\n",
        "    [df['major_category'], df['Frequency']],\n",
        "    df['ai_updated_ben']\n",
        ")\n",
        "print(summary_major)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bHjv7oE7sOQd"
      },
      "outputs": [],
      "source": [
        "# calculate proportions within each group\n",
        "prop_summary_major = summary_major.div(summary_major.sum(axis=1), axis=0)\n",
        "# convert to percentages\n",
        "prop_summary_major_percentage = (prop_summary_major * 100).round(2).astype(str) + '%'\n",
        "print(prop_summary_major_percentage)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bUtvRcY8sbQQ"
      },
      "outputs": [],
      "source": [
        "# stacked bar chart\n",
        "summary_major.plot(kind='bar', stacked=True, figsize=(10, 6))\n",
        "plt.title('AI Tool Benefits and Challenges by Major and Frequency')\n",
        "plt.ylabel('Number of Responses')\n",
        "plt.xlabel('Group (Major, Frequency)')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xiXEq2MvsngP"
      },
      "outputs": [],
      "source": [
        "# readable labels\n",
        "df['major_category'] = df['major_encoded'].map(code_to_category)\n",
        "df['Frequency'] = df['consistency_encoded'].map(frequency_labels)\n",
        "\n",
        "# Create a summary table: counts by major and frequency\n",
        "summary = pd.crosstab(df['Frequency'], df['major_category'])\n",
        "ordered_frequencies = ['Rarely / Never', 'Occasionally', 'Regularly', 'Frequently / Daily']\n",
        "summary = summary.reindex(ordered_frequencies)\n",
        "\n",
        "# Number of majors/plots\n",
        "num_majors = len(summary.columns)\n",
        "cols = 3  # number of columns in the subplot grid\n",
        "rows = -(-num_majors // cols)  # ceiling division\n",
        "\n",
        "# Create small line graphs for each major\n",
        "fig, axes = plt.subplots(rows, cols, figsize=(14, 4*rows), sharey=True)\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i, major in enumerate(summary.columns): #### switch to line graph ####\n",
        "    ax = axes[i]\n",
        "    ax.plot(summary.index, summary[major], marker='o', color='#4CAF50')\n",
        "    ax.set_title(f'{major}')\n",
        "    ax.set_xlabel('Frequency of AI Tool Use')\n",
        "    ax.set_ylabel('Number of Students')\n",
        "    ax.set_xticks(range(len(summary.index)))\n",
        "    ax.set_xticklabels(summary.index, rotation=45, ha='right')\n",
        "    ax.grid(True, axis='y')\n",
        "\n",
        "for j in range(i + 1, len(axes)):\n",
        "    fig.delaxes(axes[j])\n",
        "\n",
        "fig.suptitle('AI Tool Usage Frequency by Major (Individual Line Graphs)', fontsize=16)\n",
        "plt.tight_layout(rect=[0, 0, 0.95, 0.95])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXhYItTStDXU"
      },
      "source": [
        "These visualizations provide even more insights into AI tools in student learning and the difference in experience amongst different majors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8CxCOOTy94_"
      },
      "source": [
        "1.   Technical Majors remain the leaders of AI tool use in student learning:\n",
        "\n",
        "    *   IT (computer science, etc.) and Engineering clearly exhibit the highest use of AI tools in student learning on a frequent to daily basis\n",
        "    *   STEM fields demonstrate a steeper increase in frequent AI use compared to non-STEM majors that commonly report occasional or, at most, regular AI use"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTV8KFz7s_mt"
      },
      "source": [
        "2.   Patterns in frequency of use are inconsistent:\n",
        "\n",
        "    *   Occasional and regular use are more similar across majors\n",
        "    *   Most frequent use highly differs by major, almost no one never uses AI tools in education now"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IbKipp9fs9go"
      },
      "source": [
        "3.   Benefits are more abundant amongst STEM majors:\n",
        "\n",
        "    *   Technical majors demonstrate overwhelming benefits with their use of AI tools (i.e. efficiency, productivity, information availability)\n",
        "    *   Students in most other majors are likely to mention both advantages and challenges (i.e. confusion of platform use, concern regarding misuse)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYz5T1Sws20R"
      },
      "source": [
        "4.   AI student adoption is not universal:\n",
        "\n",
        "    *   While AI tools are being well adopted by some majors, many students out of stem at most, use AI tools with some regularity\n",
        "    *   To expand benefits and minimize challenges, interventions are necessary to support groups that utilize AI platforms less to address concerns, and across all groups to confirm AI literacy is part of the student experience"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iVKeoqIzDteI"
      },
      "source": [
        "**WHAT DOES EACH MAJOR USE AI TOOLS FOR**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lErf4LkoD0Ai"
      },
      "source": [
        "Based on the difference in AI tool use among different kinds of majors, it would be important to contextualize how different students use AI tools."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yZd6iAqkD7V5"
      },
      "outputs": [],
      "source": [
        "# ai tool task columns\n",
        "ai_task_columns = ['Writing', 'Studying', 'Brainstorming', 'Research', 'Programming', 'Nothing']\n",
        "\n",
        "# create a summary of AI tool usage by major category\n",
        "summary_by_major = df.groupby('major_category')[ai_task_columns].sum()\n",
        "\n",
        "print(\"AI Tool Usage by Major Category:\")\n",
        "print(summary_by_major)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oXFp62xFi0Md"
      },
      "outputs": [],
      "source": [
        "# calculate proportions within each group\n",
        "row_totals = summary_by_major.sum(axis=1)\n",
        "proportions_by_major = summary_by_major.div(summary_by_major.sum(axis=1), axis=0)\n",
        "# convert to percentages\n",
        "proportions_by_major_percentage = (proportions_by_major * 100).round(2).astype(str) + '%'\n",
        "print(proportions_by_major_percentage)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GtmCPuU-mEQD"
      },
      "outputs": [],
      "source": [
        "n_students = df['major_category'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EBttnJKShvZe"
      },
      "outputs": [],
      "source": [
        "# add n= annotations to x-tick labels\n",
        "n_labels = [f\"{major}\\n(n={n_students.get(major, 0)})\" for major in proportions_by_major.index]\n",
        "\n",
        "ax = proportions_by_major.plot(\n",
        "    kind='bar',\n",
        "    stacked=True,\n",
        "    figsize=(12, 7),\n",
        "    colormap='tab10'\n",
        ")\n",
        "\n",
        "ax.set_xticks(range(len(n_labels)))\n",
        "ax.set_xticklabels(n_labels, rotation=45, ha='right')\n",
        "plt.title('AI Tool Usage by Major Category (Percentage; Sample Size Shown)')\n",
        "plt.ylabel('Percentage of Students')\n",
        "plt.xlabel('major_category')\n",
        "plt.legend(title='AI Tool Task', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9zFtCcWnMGUU"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "\n",
        "# add sample sizes to major labels\n",
        "sample_sizes = df.groupby('major_category').size()\n",
        "major_with_n = [f\"{maj} (n={sample_sizes[maj]})\" for maj in proportions_by_major.index]\n",
        "proportions_by_major_labeled = proportions_by_major.copy()\n",
        "proportions_by_major_labeled.index = major_with_n\n",
        "\n",
        "# add text annotations\n",
        "sns.set(style=\"whitegrid\")\n",
        "g = sns.clustermap(\n",
        "    proportions_by_major_labeled,\n",
        "    cmap='YlOrRd',\n",
        "    annot=True,\n",
        "    fmt='.1%',\n",
        "    col_cluster=False, # set to True to cluster both axes\n",
        "    linewidth=0.5,\n",
        "    figsize=(10, 8)\n",
        ")\n",
        "plt.suptitle('AI Tool Usage Patterns by Major Category (%)\\n(Clustered by Similarity)', y=1.05)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0MyigG_0Mccw"
      },
      "outputs": [],
      "source": [
        "# find the most popular AI tool for each major\n",
        "top_tools_by_major = summary_by_major.idxmax(axis=1)\n",
        "\n",
        "print(\"Most Popular AI Tool by Major Category:\")\n",
        "for major, tool in top_tools_by_major.items():\n",
        "    count = summary_by_major.loc[major, tool]\n",
        "    print(f\"{major}: {tool} ({count} students)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNl2L0SctRJJ"
      },
      "source": [
        "These visualizations specifically related to different majors' use of AI tools reveals various patterns in the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XpCM6csgMjko"
      },
      "source": [
        "1.   Technical majors are most likely to use Programming and Research tools\n",
        "\n",
        "    *   Not just computer science but engineering majors prominently use AI to help with programming\n",
        "    *   Arts and education are more likely to use AI tools to assist in brainstorming or writing\n",
        "    *    Engineers have a nearly homogenous distribution regarding the service of their AI platforms\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zpAA_MJEtf1I"
      },
      "source": [
        "2. The heat map reveals a variety of interesting comparisons between major categories.\n",
        "    *    Brainstorming has the smallest interval range between majors and is utilized to a solid degree by majors of all categories\n",
        "    *    Practically every survey participant that studies in the field of natural sciences uses AI tools for studying purposes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WEPm4FMwtJjP"
      },
      "source": [
        "3.   Certain major classifications do not use AI tools for specific purposes whatsoever\n",
        "    *   Students pursuing education degrees present no use of AI outside of brainstorming or writing\n",
        "    *   Only one student has demonstrated that they had intentionally never used AI for a single task throughout their entire educational experience"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PiEKcRhTuooc"
      },
      "source": [
        "Overall, there are plenty of findings that are unlocked from multivariate comparisons. These help understand the sentiment and utility around AI tools currently being used in school. From here, Machine Learning applications can be used for more signficant determinations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5V1fmV00u_-O"
      },
      "source": [
        "# Machine Learning / Model Intergration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAOc-neDiF02"
      },
      "source": [
        "The first model to be applied will be Latent Dirichlet Allocation (LDA) for Natural Language Processing. This will contribute to finding themes of student sentiment with the open ended responses. The first open ended column of interest will be the benefits / challenges section."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "vgUpp8NJsLgI"
      },
      "outputs": [],
      "source": [
        "# install dependencies\n",
        "!pip install pyLDAvis scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CzRLvlXysRvL"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "\n",
        "texts = df['ai_benefit_clean'].dropna().tolist()\n",
        "vectorizer = CountVectorizer(stop_words='english', max_df=0.95, min_df=2)\n",
        "dtm = vectorizer.fit_transform(texts)\n",
        "\n",
        "num_topics = 5\n",
        "lda = LatentDirichletAllocation(n_components=num_topics, random_state=0)\n",
        "lda.fit(dtm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P5dmjV8wtVYs"
      },
      "outputs": [],
      "source": [
        "import pyLDAvis.lda_model\n",
        "\n",
        "panel = pyLDAvis.lda_model.prepare(lda, dtm, vectorizer)\n",
        "\n",
        "pyLDAvis.display(panel)\n",
        "pyLDAvis.enable_notebook()\n",
        "pyLDAvis.display(panel)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fj8NiDaUxcj4"
      },
      "source": [
        "The LDA visualization reveals some important information regarding the general perceived topics in the benefits / concerns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GiNj-0DvxibK"
      },
      "source": [
        "1. The Intertopic Distance Map, *top left*, visually distinguishes the primary themes discussed in student responses\n",
        "\n",
        "    *   The larger the circle, the more relevant it is through the responses. The further the distance between circles relative to each other, the most distinct they are through an LDA lense\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oo1bc7tByRp4"
      },
      "source": [
        "2. Salient terms are common words throughout the responses that identify the individual primary themes.\n",
        "    *   Words including \"helps\" or \"easier\" indicate beneficial use\n",
        "    *   Words including \"complex\" or \"difficult\" may imply challenges without applied context"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iEZZZW0Ny2-o"
      },
      "source": [
        "3. Various words including \"structure\" and \"streamlined\" demonstrate specific way that students actively engage with AI tools\n",
        "    *   As mentioned previously, certain words may reflect issues where students need additional support in specific regards"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VE7rVS9xBlQk"
      },
      "source": [
        "Next, we will apply the open ended responses about suggestions going forward from students about the university utilizing AI tools."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wErRtfC8Bum-"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "\n",
        "texts = df['ai_suggestions_clean'].dropna().tolist()\n",
        "vectorizer = CountVectorizer(stop_words='english', max_df=0.95, min_df=2)\n",
        "dtm = vectorizer.fit_transform(texts)\n",
        "\n",
        "num_topics = 5\n",
        "lda = LatentDirichletAllocation(n_components=num_topics, random_state=0)\n",
        "lda.fit(dtm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mxTERmbQB7c0"
      },
      "outputs": [],
      "source": [
        "import pyLDAvis.lda_model\n",
        "\n",
        "panel = pyLDAvis.lda_model.prepare(lda, dtm, vectorizer)\n",
        "\n",
        "pyLDAvis.display(panel)\n",
        "pyLDAvis.enable_notebook()\n",
        "pyLDAvis.display(panel)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_bX9QbAHs4L"
      },
      "source": [
        "Student suggestions demonstrate significant positive impacts and concerns regarding student learning. There primary topics presented demonstrate different components of the student experience with AI tools."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWkHKHmmH7HG"
      },
      "source": [
        "1. Efficiency and convenience appear to be the most salient components of the analysis.\n",
        "    *   Students commonly highlight how AI tools help them to complete assignments more quickly, provides brainstorming techniques for research and essays, and allowing quick access to a variety of information and knowledge\n",
        "    *   Brainstorming is key for many students; they find opportunity for this to allow for new approaches to problem solving and decision making\n",
        "    *   Another common benefit is how the accessibility of these platforms allows for more availability of academic resources"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QtyMjJonJMee"
      },
      "source": [
        "2. Concerns permiate through all categories created by the LDA model\n",
        "    *  The most addressed concern appears to be that AI tools are providing misleading or inaccurate information to students\n",
        "    *  Another common issue is the potential for AI to increase current academic problems such as plagerism and fairness with the use of AI generated contents\n",
        "    *  A somewhat consistent acknowledgement is certain students overusing these platforms to a degree that fundamentally affects critical thinking skills and originality"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9WxUFVtKjj6"
      },
      "source": [
        "Main takeaways for students from LDA:\n",
        "\n",
        "*   Students should use AI for support, but tnot a substitue to apply their own judgement and individual approach\n",
        "*   Be sure to review AI generated material by fact checking its related resources to make sure the output is accurate\n",
        "*   Work within instituional guidelines of AI use to avoid  academic wrongdoing and to not sacrifice ones student learning experience\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJ6hBFYUMFJU"
      },
      "source": [
        "Main takeaways from administrators from LDA:\n",
        "\n",
        "\n",
        "    *   List item\n",
        "    *   List item\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUWlShSMV38O"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "680fa030"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}